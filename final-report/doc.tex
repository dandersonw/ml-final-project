\documentclass{article}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{cite}
\usepackage{listings}
\usepackage{multicol}
\usepackage{url}
\usepackage{xeCJK}
\setCJKmainfont{IPAMincho}

\setlength{\parindent}{4em}
\setlength{\parskip}{1em}

\title{Transliterating English to Japanese Without Parallel Data}
\date{2019-04-22}
\author{Derick Anderson \\ anderson.de@husky.neu.edu
  \and Timothy Gillis \\ gillis.ti@husky.neu.edu }

\begin{document}

\pagenumbering{gobble}
\maketitle

\section*{Introduction}

We aim to learn to transliterate English to Japanese
without any parallel data.
That is to say,
given only monolingual corpora
and the tiniest insight about Japanese orthography
to learn how to represent English words in Japanese script.
Learning to transliterate with limited supervision has been well studied,
but there have been only limited attempts to learn without at least parallel data.
The key inspiration was the recent paper by
Conneau et al. \cite{Conneau2018WordTW}
in which word translations were learned with no parallel data
(or any other supervision).
Conneau et al. did not touch Japanese,
so we hope to first replicate their work on the Japanese-English language pair.
We then hope to discover an algorithm to learn to transliterate
using those (probably noisy) results as training data.

As big data becomes more readily available, the cost of manually labeling large
datasets emphasizes the need for unsupervised algorithms that perform as well if
not better than their supervised counterparts, and the cost of cleaning these
datasets highlights the need for algorithms to effectively handle noisy data as well.
Our project will touch upon both of these problems in the machine transliteration
sphere, specifically by first generating a Japanese-English dictionary in an
unsupervised way, then attempting to learn to transliterate from that noisy data.

Derick has a personal interest in Japanese, transliteration, and both together.
Timothy would like to gain experience in natural language processing and machine
translation.

\section*{Background}

\subsection*{Transliteration}

Transliteration is representing words
in a script or orthographic style
other than that with which they were originally represented.
Between close scripts like the Old English alphabet and the modern English alphabet
this can be a mechanical character to character mapping:
e.g. ``þe olde'' to ``ye olde''.
Between distant scripts
like the English alphabet and Japanese katakana
the task becomes more difficult.
Translation includes transliteration as a subtask,
although the relationship between the two
depends on the language pair, intended audience, and other factors.

In the literature on machine transliteration there is a distinction
between generative transliteration and transliteration extraction.
Generative transliteration learns a function for transliterating;
transliteration extraction just identifies pairs of strings
to be added to a transliteration dictionary
\cite{Karimi:2011:MTS:1922649.1922654}.
In this paper we will be doing generative transliteration.

In translating into both English and Japanese
probably the most common use of transliteration
is representing foreign proper nouns.
Because proper nouns are an open and varied class
there has been a lot of work on unsupervised learning to transliterate them
(e.g. \cite{Tao2006UnsupervisedNE}),
maybe for use in machine translation (e.g. \cite{Durrani2014IntegratingAU})
or cross-language information retrieval (e.g. \cite{10.1007/978-3-642-40087-2_29}).
A limitation of our proposed approach
is that we will not pay any special attention to proper nouns,
which often have their own conventions for transliteration.
% maybe we will want to?

The use of transliteration most relevant to this project
is the borrowing of words from foreign languages.
Like proper nouns,
some words may not have equivalents in every language,
and rather than create a neologism from native material (e.g. as done in Icelandic)
a foreign word might be adopted and used.

The work most closely related to ours is that of Ravi and Knight
\cite{Ravi2009LearningPM}
who learn to back-transliterate from Japanese to English
without any parallel data.
Back-transliteration is the process of
transforming a katakana string that represents an English word
back into the English word it represents.
The Ravi and Knight work seems to be followed up some other work
like a paper by Levinboim et. al. \cite{Levinboim2015ModelIR}
which aims to improve the same basic approach.
The key difference between their approach and ours
is that theirs requires a well trained model of Japanese and English phonetics.
We may choose to utilize an English phonetic model,
but don't require either.
Furthermore,
they impose constraints and start with favorable intializations
based on prior knowledge about transliteration.
We will utilize the fact
that English words are translated into katakana,
but hope to not inject any other prior knowledge.
Learning to transliterate into katakana
without knowledge of their pronunciation
or much aid from prior knowledge should be a step forward.

\subsection*{The Japanese-English Language Pair}

In modern orthography
Japanese represents foreign words
by approximating the pronunciation of the foreign word
in a syllabary \footnote{In a syllabary each character represents a syllable,
although katakana has some exceptions by that definition.} called katakana.
Native Japanese words and loanwords from Chinese
\footnote{Words of Sinitic origin are so common in Japanese that they not considered
  foreign in the same sense as words from English.}
are usually represented in scripts besides katakana:
the hiragana syllabary and kanji
\footnote{``Kanji'' is the Japanese name for Chinese characters.}.
The approximation of pronunciation is really quite approximate;
because Japanese has a relatively limited phonetic inventory (range of sounds used)
not all English sounds can be represented.
In the face of the complicated correspondence
between English spelling and English pronunciation
(to say nothing of international English variants)
Japanese people sometimes transliterate based on the spelling of a word
rather than the pronunciation.
An ideal system,
therefore,
will probably need to consider both phonetics and spelling.

Japanese has taken many loan words from English.
These can be common nouns (bed, ベッド),
verbs (join, ジョイン),
adjectives (sexy, セクシー),
or even sentence pieces (let's!, レッツ！).
Many (most?) loan words
are used in the same ways as their English equivalents,
although the meaning of some has diverged.
Japanese has also taken loanwords from other languages
that are written in katakana,
some of which may conflict with English words.
An example is ナトリウム,
from the Latin natrium,
meaning sodium (consider the Na elemental abbreviation).
The hope is that clever matching of English words
will allow useful transcription pairs to be found.

\subsection*{Word Translation Without Parallel Data}

Last year, Conneau et al. proposed a state-of-the-art unsupervised method for machine
translation without using any parallel corpora, that works well for both distant
language pairs (e.g. English-Chinese, English-Japanese) and pairs with limited parallel
data (e.g. English-Esperanto), also outperforming supervised methods on some language
pairs. This is accomplished by aligning the monolingual word embedding spaces for each
language in the pair.

Supervised methods attempt to learn a linear mapping $W$ between the source and target
space such that $WX-Y$ is minimized, where $X$ and $Y$ are the source and target word
embedding pairs, respectively. Because they attempt to learn $W$ without cross-lingual 
supervision, they use a domain-adversarial approach, where a discriminator is trained
to discriminate between translated (i.e. generated) words and words actually from the
target domain, and $W$ is simultaneously trained to "fool" the discriminator by making
the generated words as similar to the target words as possible.

While the learned $W$ roughly aligns the two embedding spaces, Conneau et al add two
modifications to further align the spaces:

First, a refinement procedure is proposed where the $W$ trained via the
adversarial method is used to generate a synthetic parallel vocabulary, which is
then used to minimize $WX-Y$ in a supervised manner. More specifically, they use
the generated parallel vocabulary to apply the Procrustes solution (i.e.
min $\|WX-Y\|_2 = UV^T$, where $U\Sigma V^T = SVD(YX^T)$). This is repeated several
times, with the expectation being that the generated data improves each iteration.
This approach can also be "kickstarted" in a supervised manner by simply applying
the first iteration with a ground truth parallel vocabulary.

Second, they introduce a cross-domain similarity adaptation, cross-domain similarity
local scaling (CSLS), to mitigate the "hubness problem"
(i.e. in high dimensional spaces, some points tend to be nearest neighbors
to many other points). Specifically, they use the mean similarity of a source
embedding translation ($Wx_s$) to its target K nearest neighbors:
$$
r_T(Wx_s)=\frac{1}{K}\sum_{y_t \in \mathcal{N}_T(Wx_s)} \text{cos}(Wx_s, y_t)
$$
where $\mathcal{N}_T(Wx_s)$ is the K nearest neighbors of the source embedding
translation, and define the similarity metric as:
$$
\text{CSLS}(Wx_s,y_t)=2\text{cos}(Wx_s, y_t)-r_T(Wx_s)-r_S(y_t)
$$

\section*{Methods}

\subsection*{Unsupervised Word Translation}

\subsection*{Data}

We begin by downloading pre-trained monolingual word vectors for English, Japanese,
and all the other languages we plan to run experiments with \cite{bojanowski2017enriching}.
These are 300 dimensional word vectors trained on Wikipedia using fastText.
Although this project's overall goal is to learn transliteration for the specific
English-Japanese language pair, for this section we plan to more generally analyze
how well the approach described in Word Translation Without Parallel Data generalizes
to new language pairs, especially one of languages that are comparably as distant as
the English-Japanese pair.

Other than Japanese, we run experiments on the following languages paired with English:
Spanish, French, Chinese, Portuguese, Hebrew, Hindi, Thai, Korean, and Arabic. This
allows us to first reproduce their results on 3 language pairs, along with test their
method's generalization on 7 new language pairs of varying similarity.

\subsection*{Methodology}

As we are simply reproducing the results outlined in Word Translation Without Parallel
Data, we use the same training architecture and methodology. Firstly, only the 200k
most frequent words are loaded for training the mapping matrix $W$, and the 50k most
frequent for the discriminator. A two layered feed forward neural network is used
for the discriminator, with each hidden layer of size 2048, Leaky-ReLU activation
functions and a dropout rate of 0.1. The default training parameters for
both the discriminator and $W$ as described in the paper are a
batch size of 32, learning rate of 0.1, learning rate decay of 0.95, smoothing
coefficient of 0.2, epoch size of 250k and a learning rate shrink parameter of 0.5
(i.e. whenever the unsupervised validation criterion decreases after a training epoch
they divide the learning rate in half). Since we find that their proposed validation
criterion does not correlate as well with word translation precision on new language
pairs, we change this shrink parameter to 0.75.

For the mapping matrix $W$'s update rule, an orthogonal constrant is imposed, where
$\beta=0.01$:
$$
W \leftarrow (1 + \beta)W - \beta(WW^T)W
$$

When generating the synthetic parallel data to be used for transliteration, we take
the aligned Japanese embedding space with the best word translation precision
@ $\text{k}=1$ and generate proposed translations for the 50k most frequent
source words using CSLS. We choose the top 50k words since that is the largest
amount our computer can handle to perform CSLS on at the same time. While we could
run vanilla K nearest neighbors in "batches", this produces worse translations
since the hubness problem persists.

\subsection*{Evaluation}

The proposed validation metric CSLS is used with the 10k most frequent source words
for model selection (and when deciding to shrink the learning rate during training).
However for final evaluation, we simply use word translation precision @
$\text{k}=1$. This is to ensure the translations used for transliteration are as
accurate as possible.

\subsection*{Transliteration}

\subsubsection*{Data}

We use a few external sources of data.
Facebook Research provides a ground truth English-Japanese dictionary
of the same sort as is used for their evaluation of word translation,
available on their GitHub
\footnote{https://github.com/facebookresearch/MUSE}.
It contains about 14,000 translations of an English word to a katakana string.
We used this dictionary for evaluating the transliteration model
we learn on generated pairs.
Our thinking was that Facebook's ground truth dictionary
should be alike to the pairs we hope to generate from word translation
using their method.
We will refer to it as the ``muse'' data.
A concrete way in which that is true is that
there are no translation pairs with two English words,
unlike some other transliteration data available.
We use the dataset available here
\footnote{https://github.com/eob/english-japanese-transliteration}
because it is freely available,
of decent quality,
and referenced in the Google paper\cite{Rosca2016SequencetosequenceNN}
we use as a baseline.
We will refer to it as the ``eob'' data.
CMU makes available a large dataset of English words and their pronunciations
\footnote{http://www.speech.cs.cmu.edu/cgi-bin/cmudict},
which we will refer to as the ``cmu'' data.

\subsubsection*{Learning Transliteration: Basic}

Considering just the bird's eye view of the task
as transforming a sequence of characters to another sequence of characters,
our first thought was to use a sequence-to-sequence model
like the well known Google neural machine translation model
\cite{Wu2016GooglesNM}.
Not surprisingly,
some people at Google did try that \cite{Rosca2016SequencetosequenceNN}
and found that the results were highly competitive.
Since attentional sequence-to-sequence models \cite{Bahdanau2015NeuralMT}
are so well known,
we won't describe the basic architecture here.

Treating words as just sequences of characters
diverges from a lot of transliteration methods
in ignoring pronunciation.
The motivation for that
is that we don't have pronunciation information for translation-pairs
that we get out of the word translation half of the project.

All of the networks we use operate on a character basis.
Each script is a separate vocabulary;
the Japanese decoder can output only katakana characters.
We operate on composed representations of Unicode characters:
e.g. ``ガ'' is represented by the atomic \texttt{U+X30AC KATAKANA LETTER GA},
not ``カ'' \texttt{U+X30AB KATAKANA LETTER KA} followed by
``゛''\texttt{U+X3099 COMBINING KATAKANA-HIRAGANA VOICED-SOUND MARK}.

\subsubsection*{Learning Transliteration: Multitask with Pronunciation}

We note above our intuition that the ideal transliteration model
must consider both the spelling and pronunciation of an English word.
Coincidentally,
the authors of the Google neural transliteration paper
\cite{Rosca2016SequencetosequenceNN}
note that they too think it would be a good idea
to feed the transliteration model pronunciation information.

We chose to achieve that not by
explicitly integrating a pronunciation layer into our model,
but by formulating a multitask learning problem.
The first task is to predict the pronunciation of an English word,
and the second to predict the transliteration of an English word.
We use the same basic sequence-to-sequence architecture for both problems.

% TODO...

\subsubsection*{Evaluation}

As mentioned in the data section,
we evaluate the transliteration model learned on our generated word translations
on translations pairs excerpted from an English-Japanese dictionary
provided by Facebook research.
For the other transliteration models
we hold out 10\% of the eob data as a test set.

Our personal opinion is that evaluating the correctness
of a transliterator is not that straightforward.
Nevertheless,
we will use a few straightforward measures
that appear in the transliteration literature.
The first is simply accuracy:
for what proportion of the input words
does the transliterator return the correct transliteration.
The second is Levenshtein distance:
the number of simple edits necessary to change the output to
the correct transliteration
\footnote{https://en.wikipedia.org/wiki/Levenshtein\_distance}.
The third is mean reciprocal rank (MRR):
a measure of how far down the list of possible results
(ordered in direction of decreasing probability)
the correct answer is
\footnote{https://en.wikipedia.org/wiki/Mean\_reciprocal\_rank}.

Typically Levenshtein distance is normalized in a few ways
when evaluating transliteration models.
The first way is normalizing by the length of the ground-truth answer,
to get a character error rate (CER):
$$CER(gold, predicted) = \frac{Levenshtein(gold, predicted)}{length(gold)}$$.
Word error rate (WER) is the same as CER,
except it is calculated on a word-level instead of character-level.
It is used by the Google neural transliteration paper,
although in the case of evaluating katakana transliterations
``word-level'' is really problematic
(there is no trivial way to segment katakana strings into words).
We will instead consider accuracy,
on the basis that $1 - WER$
is a generous but approximately reasonable estimate of accuracy.

Although it is not connected to any of the papers we cite,
we also report a modified CER,
which we denote CER*.
It is modified in that the Levenshtein distance
is adjusted to better represent the abstract distance between katakana strings.
For example,
we say that it costs only one half a point
to insert a gemination (consonant-doubling) mark.

\section*{Results}

\subsection*{Unsupervised Word Translation}

\subsection*{Transliteration}

\section*{Conclusions}

\section*{Individual Tasks}

\bibliography{doc}{}
\bibliographystyle{plain}
\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-engine: xetex
%%% TeX-master: t
%%% End:
