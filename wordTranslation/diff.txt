diff --git a/evaluate.py b/evaluate.py
index e49da01..ed3ad89 100644
--- a/evaluate.py
+++ b/evaluate.py
@@ -9,10 +9,11 @@
 
 import os
 import argparse
+import csv
 from collections import OrderedDict
 
 from src.utils import bool_flag, initialize_exp
-from src.models import build_model
+from src.models import build_model, update_src_embeddings
 from src.trainer import Trainer
 from src.evaluation import Evaluator
 
@@ -24,13 +25,13 @@ parser.add_argument("--exp_name", type=str, default="debug", help="Experiment na
 parser.add_argument("--exp_id", type=str, default="", help="Experiment ID")
 parser.add_argument("--cuda", type=bool_flag, default=True, help="Run on GPU")
 # data
-parser.add_argument("--src_lang", type=str, default="", help="Source language")
-parser.add_argument("--tgt_lang", type=str, default="", help="Target language")
+parser.add_argument("--src_lang", type=str, default="ja", help="Source language")
+parser.add_argument("--tgt_lang", type=str, default="en", help="Target language")
 parser.add_argument("--dico_eval", type=str, default="default", help="Path to evaluation dictionary")
 # reload pre-trained embeddings
 parser.add_argument("--src_emb", type=str, default="", help="Reload source embeddings")
 parser.add_argument("--tgt_emb", type=str, default="", help="Reload target embeddings")
-parser.add_argument("--max_vocab", type=int, default=200000, help="Maximum vocabulary size (-1 to disable)")
+parser.add_argument("--max_vocab", type=int, default=-1, help="Maximum vocabulary size (-1 to disable)")
 parser.add_argument("--emb_dim", type=int, default=300, help="Embedding dimension")
 parser.add_argument("--normalize_embeddings", type=str, default="", help="Normalize embeddings before training")
 
@@ -46,16 +47,39 @@ assert params.dico_eval == 'default' or os.path.isfile(params.dico_eval)
 
 # build logger / model / trainer / evaluator
 logger = initialize_exp(params)
-src_emb, tgt_emb, mapping, _ = build_model(params, False)
+src_emb, tgt_emb, mapping, _, srcWord2FreqRank = build_model(params, False, 1)
 trainer = Trainer(src_emb, tgt_emb, mapping, None, params)
 evaluator = Evaluator(trainer)
 
 # run evaluations
 to_log = OrderedDict({'n_iter': 0})
-evaluator.monolingual_wordsim(to_log)
-# evaluator.monolingual_wordanalogy(to_log)
 if params.tgt_lang:
-    evaluator.crosslingual_wordsim(to_log)
-    evaluator.word_translation(to_log)
-    evaluator.sent_translation(to_log)
-    # evaluator.dist_mean_cosine(to_log)
+
+    emb_idx = 1
+    while True:
+        src_word_ids, matches = evaluator.word_translation(to_log)
+
+        with open('matches_500k_4500_batch.csv', 'a') as fp:
+            wr = csv.writer(fp, delimiter=',')
+            for idx, m in enumerate(matches):
+                word = evaluator.src_dico.id2word[src_word_ids[idx]]
+    
+                line = []
+                line.append(word)
+                for word_id in m:
+                    line.append(evaluator.tgt_dico.id2word[word_id])
+                line.append(srcWord2FreqRank[word])
+    
+                wr.writerow(line)
+
+        del trainer
+        del evaluator
+
+        emb_idx += 4500
+
+        if emb_idx > 200312:
+            break
+
+        srcWord2FreqRank = update_src_embeddings(src_emb, params, emb_idx)
+        trainer = Trainer(src_emb, tgt_emb, mapping, None, params)
+        evaluator = Evaluator(trainer)
diff --git a/gen_train_set.py b/gen_train_set.py
new file mode 100644
index 0000000..4167abf
--- /dev/null
+++ b/gen_train_set.py
@@ -0,0 +1,101 @@
+# Copyright (c) 2017-present, Facebook, Inc.
+# All rights reserved.
+#
+# This source code is licensed under the license found in the
+# LICENSE file in the root directory of this source tree.
+#
+
+import os
+import time
+import json
+import argparse
+from collections import OrderedDict
+import numpy as np
+import torch
+
+from src.utils import bool_flag, initialize_exp
+from src.models import build_model
+from src.trainer import Trainer
+from src.evaluation import Evaluator
+
+
+VALIDATION_METRIC = 'mean_cosine-csls_knn_10-S2T-10000'
+
+
+# main
+parser = argparse.ArgumentParser(description='Unsupervised training')
+parser.add_argument("--seed", type=int, default=4242, help="Initialization seed")
+parser.add_argument("--verbose", type=int, default=2, help="Verbose level (2:debug, 1:info, 0:warning)")
+parser.add_argument("--exp_path", type=str, default="", help="Where to store experiment logs and models")
+parser.add_argument("--exp_name", type=str, default="debug", help="Experiment name")
+parser.add_argument("--exp_id", type=str, default="", help="Experiment ID")
+parser.add_argument("--cuda", type=bool_flag, default=True, help="Run on GPU")
+parser.add_argument("--export", type=str, default="txt", help="Export embeddings after training (txt / pth)")
+# data
+parser.add_argument("--src_lang", type=str, default='en', help="Source language")
+parser.add_argument("--tgt_lang", type=str, default='ja', help="Target language")
+parser.add_argument("--emb_dim", type=int, default=300, help="Embedding dimension")
+parser.add_argument("--max_vocab", type=int, default=200000, help="Maximum vocabulary size (-1 to disable)")
+# mapping
+parser.add_argument("--map_id_init", type=bool_flag, default=True, help="Initialize the mapping as an identity matrix")
+parser.add_argument("--map_beta", type=float, default=0.001, help="Beta for orthogonalization")
+# discriminator
+parser.add_argument("--dis_layers", type=int, default=2, help="Discriminator layers")
+parser.add_argument("--dis_hid_dim", type=int, default=2048, help="Discriminator hidden layer dimensions")
+parser.add_argument("--dis_dropout", type=float, default=0., help="Discriminator dropout")
+parser.add_argument("--dis_input_dropout", type=float, default=0.1, help="Discriminator input dropout")
+parser.add_argument("--dis_steps", type=int, default=5, help="Discriminator steps")
+parser.add_argument("--dis_lambda", type=float, default=1, help="Discriminator loss feedback coefficient")
+parser.add_argument("--dis_most_frequent", type=int, default=75000, help="Select embeddings of the k most frequent words for discrimination (0 to disable)")
+parser.add_argument("--dis_smooth", type=float, default=0.1, help="Discriminator smooth predictions")
+parser.add_argument("--dis_clip_weights", type=float, default=0, help="Clip discriminator weights (0 to disable)")
+# training adversarial
+parser.add_argument("--adversarial", type=bool_flag, default=True, help="Use adversarial training")
+parser.add_argument("--n_epochs", type=int, default=5, help="Number of epochs")
+parser.add_argument("--epoch_size", type=int, default=1000000, help="Iterations per epoch")
+parser.add_argument("--batch_size", type=int, default=32, help="Batch size")
+parser.add_argument("--map_optimizer", type=str, default="sgd,lr=0.2", help="Mapping optimizer")
+parser.add_argument("--dis_optimizer", type=str, default="sgd,lr=0.2", help="Discriminator optimizer")
+parser.add_argument("--lr_decay", type=float, default=0.95, help="Learning rate decay (SGD only)")
+parser.add_argument("--min_lr", type=float, default=1e-6, help="Minimum learning rate (SGD only)")
+parser.add_argument("--lr_shrink", type=float, default=1, help="Shrink the learning rate if the validation metric decreases (1 to disable)")
+# training refinement
+parser.add_argument("--n_refinement", type=int, default=5, help="Number of refinement iterations (0 to disable the refinement procedure)")
+# dictionary creation parameters (for refinement)
+parser.add_argument("--dico_eval", type=str, default="default", help="Path to evaluation dictionary")
+parser.add_argument("--dico_method", type=str, default='csls_knn_10', help="Method used for dictionary generation (nn/invsm_beta_30/csls_knn_10)")
+parser.add_argument("--dico_build", type=str, default='S2T', help="S2T,T2S,S2T|T2S,S2T&T2S")
+parser.add_argument("--dico_threshold", type=float, default=0, help="Threshold confidence for dictionary generation")
+parser.add_argument("--dico_max_rank", type=int, default=15000, help="Maximum dictionary words rank (0 to disable)")
+parser.add_argument("--dico_min_size", type=int, default=0, help="Minimum generated dictionary size (0 to disable)")
+parser.add_argument("--dico_max_size", type=int, default=0, help="Maximum generated dictionary size (0 to disable)")
+# reload pre-trained embeddings
+parser.add_argument("--src_emb", type=str, default="wiki.en.vec", help="Reload source embeddings")
+parser.add_argument("--tgt_emb", type=str, default="wiki.ja.vec", help="Reload target embeddings")
+parser.add_argument("--normalize_embeddings", type=str, default="", help="Normalize embeddings before training")
+
+
+# parse parameters
+params = parser.parse_args()
+
+## check parameters
+#assert not params.cuda or torch.cuda.is_available()
+#assert 0 <= params.dis_dropout < 1
+#assert 0 <= params.dis_input_dropout < 1
+#assert 0 <= params.dis_smooth < 0.5
+#assert params.dis_lambda > 0 and params.dis_steps > 0
+#assert 0 < params.lr_shrink <= 1
+#assert os.path.isfile(params.src_emb)
+#assert os.path.isfile(params.tgt_emb)
+##assert params.dico_eval == 'default' or os.path.isfile(params.dico_eval)
+#assert params.export in ["", "txt", "pth"]
+#
+## build model / trainer / evaluator
+#logger = initialize_exp(params)
+#
+#src_emb, tgt_emb, mapping, discriminator = build_model(params, True)
+#trainer = Trainer(src_emb, tgt_emb, mapping, discriminator, params)
+#
+#trainer.build_dictionary()
+#
+#print(trainer.dico)
diff --git a/run_experiments.sh b/run_experiments.sh
new file mode 100755
index 0000000..f0e3834
--- /dev/null
+++ b/run_experiments.sh
@@ -0,0 +1,84 @@
+#!/bin/bash
+
+## ES-EN
+#python unsupervised.py --src_lang es --tgt_lang en --src_emb wiki.es.vec --tgt_emb wiki.en.vec --n_refinement 8 --n_epochs 10 --epoch_size 250000 --normalize_embeddings center --lr_shrink 0.75 --exp_name spanish
+#
+## FR-EN
+#python unsupervised.py --src_lang fr --tgt_lang en --src_emb wiki.fr.vec --tgt_emb wiki.en.vec --n_refinement 8 --n_epochs 10 --epoch_size 250000 --normalize_embeddings center --lr_shrink 0.75 --exp_name french
+#
+## ZH-EN
+#python unsupervised.py --src_lang zh --tgt_lang en --src_emb wiki.zh.vec --tgt_emb wiki.en.vec --n_refinement 8 --n_epochs 10 --epoch_size 250000 --normalize_embeddings center --lr_shrink 0.75 --exp_name chinese
+#
+## KO-EN
+#python unsupervised.py --src_lang ko --tgt_lang en --src_emb wiki.ko.vec --tgt_emb wiki.en.vec --n_refinement 8 --n_epochs 10 --epoch_size 250000 --normalize_embeddings center --lr_shrink 0.75 --exp_name korean
+# JA-EN
+#python unsupervised.py --src_lang ja --tgt_lang en --src_emb wiki.ja.vec --tgt_emb wiki.en.vec --n_refinement 8 --n_epochs 10 --epoch_size 250000 --normalize_embeddings center --lr_shrink 0.75 --exp_name korean
+
+#python supervised.py --src_lang ja --tgt_lang en --src_emb wiki.ja.vec --tgt_emb wiki.en.vec --n_refinement 8 --dico_train default --exp_name japanese_supervised
+#python supervised.py --src_lang ko --tgt_lang en --src_emb wiki.ko.vec --tgt_emb wiki.en.vec --n_refinement 8 --dico_train default --exp_name korean_supervised
+#python supervised.py --src_lang zh --tgt_lang en --src_emb wiki.zh.vec --tgt_emb wiki.en.vec --n_refinement 8 --dico_train default --exp_name chinese_supervised
+#python supervised.py --src_lang fr --tgt_lang en --src_emb wiki.fr.vec --tgt_emb wiki.en.vec --n_refinement 8 --dico_train default --exp_name french_supervised
+
+# EN-FR
+#python unsupervised.py --src_lang en --tgt_lang fr --src_emb wiki.en.vec --tgt_emb wiki.fr.vec --n_refinement 8 --n_epochs 10 --epoch_size 250000 --normalize_embeddings center --lr_shrink 0.75 --exp_name english_french
+#python supervised.py --src_lang en --tgt_lang fr --src_emb wiki.en.vec --tgt_emb wiki.fr.vec --n_refinement 8 --dico_train default --exp_name english_french_supervised
+#
+## PT-EN
+#python unsupervised.py --src_lang pt --tgt_lang en --src_emb wiki.pt.vec --tgt_emb wiki.en.vec --n_refinement 8 --n_epochs 10 --epoch_size 250000 --normalize_embeddings center --lr_shrink 0.75 --exp_name portuguese
+#python supervised.py --src_lang pt --tgt_lang en --src_emb wiki.pt.vec --tgt_emb wiki.en.vec --n_refinement 8 --dico_train default --exp_name portuguese_supervised
+#
+## EN-PT
+#python unsupervised.py --src_lang en --tgt_lang pt --src_emb wiki.en.vec --tgt_emb wiki.pt.vec --n_refinement 8 --n_epochs 10 --epoch_size 250000 --normalize_embeddings center --lr_shrink 0.75 --exp_name english_portuguese
+#python supervised.py --src_lang en --tgt_lang pt --src_emb wiki.en.vec --tgt_emb wiki.pt.vec --n_refinement 8 --dico_train default --exp_name english_portuguese_supervised
+#
+## EN-HE
+#python unsupervised.py --src_lang en --tgt_lang he --src_emb wiki.en.vec --tgt_emb wiki.he.vec --n_refinement 8 --n_epochs 10 --epoch_size 250000 --normalize_embeddings center --lr_shrink 0.75 --exp_name english_hebrew
+#python supervised.py --src_lang en --tgt_lang he --src_emb wiki.en.vec --tgt_emb wiki.he.vec --n_refinement 8 --dico_train default --exp_name english_hebrew_supervised
+#
+## HE-EN
+#python unsupervised.py --src_lang he --tgt_lang en --src_emb wiki.he.vec --tgt_emb wiki.en.vec --n_refinement 8 --n_epochs 10 --epoch_size 250000 --normalize_embeddings center --lr_shrink 0.75 --exp_name hebrew
+#python supervised.py --src_lang he --tgt_lang en --src_emb wiki.he.vec --tgt_emb wiki.en.vec --n_refinement 8 --dico_train default --exp_name hebrew_supervised
+#
+## EN-HI
+#python unsupervised.py --src_lang en --tgt_lang hi --src_emb wiki.en.vec --tgt_emb wiki.hi.vec --n_refinement 8 --n_epochs 10 --epoch_size 250000 --normalize_embeddings center --lr_shrink 0.75 --exp_name english_hindi
+#python supervised.py --src_lang en --tgt_lang hi --src_emb wiki.en.vec --tgt_emb wiki.hi.vec --n_refinement 8 --dico_train default --exp_name english_hindi_supervised
+#
+## HI-EN
+#python unsupervised.py --src_lang hi --tgt_lang en --src_emb wiki.hi.vec --tgt_emb wiki.en.vec --n_refinement 8 --n_epochs 10 --epoch_size 250000 --normalize_embeddings center --lr_shrink 0.75 --exp_name hindi
+#python supervised.py --src_lang hi --tgt_lang en --src_emb wiki.hi.vec --tgt_emb wiki.en.vec --n_refinement 8 --dico_train default --exp_name hindi_supervised
+#
+## EN-AR
+#python unsupervised.py --src_lang en --tgt_lang ar --src_emb wiki.en.vec --tgt_emb wiki.ar.vec --n_refinement 8 --n_epochs 10 --epoch_size 250000 --normalize_embeddings center --lr_shrink 0.75 --exp_name english_arabic
+#python supervised.py --src_lang en --tgt_lang ar --src_emb wiki.en.vec --tgt_emb wiki.ar.vec --n_refinement 8 --dico_train default --exp_name english_arabic_supervised
+#
+## AR-EN
+#python unsupervised.py --src_lang ar --tgt_lang en --src_emb wiki.ar.vec --tgt_emb wiki.en.vec --n_refinement 8 --n_epochs 10 --epoch_size 250000 --normalize_embeddings center --lr_shrink 0.75 --exp_name arabic
+#python supervised.py --src_lang ar --tgt_lang en --src_emb wiki.ar.vec --tgt_emb wiki.en.vec --n_refinement 8 --dico_train default --exp_name arabic_supervised
+#
+## EN-TH
+#python unsupervised.py --src_lang en --tgt_lang th --src_emb wiki.en.vec --tgt_emb wiki.th.vec --n_refinement 8 --n_epochs 10 --epoch_size 250000 --normalize_embeddings center --lr_shrink 0.75 --exp_name english_thai
+#python supervised.py --src_lang en --tgt_lang th --src_emb wiki.en.vec --tgt_emb wiki.th.vec --n_refinement 8 --dico_train default --exp_name english_thai_supervised
+#
+## TH-EN
+#python unsupervised.py --src_lang th --tgt_lang en --src_emb wiki.th.vec --tgt_emb wiki.en.vec --n_refinement 8 --n_epochs 10 --epoch_size 250000 --normalize_embeddings center --lr_shrink 0.75 --exp_name thai
+#python supervised.py --src_lang th --tgt_lang en --src_emb wiki.ar.vec --tgt_emb wiki.en.vec --n_refinement 8 --dico_train default --exp_name thai_supervised
+
+## ES-EN
+#python supervised.py --src_lang es --tgt_lang en --src_emb wiki.es.vec --tgt_emb wiki.en.vec --n_refinement 8 --dico_train default --exp_name spanish_supervised
+#
+## EN-KO
+#python supervised.py --src_lang en --tgt_lang ko --src_emb wiki.en.vec --tgt_emb wiki.ko.vec --n_refinement 8 --dico_train default --exp_name english_korean_supervised
+#
+## EN-JA
+#python unsupervised.py --src_lang en --tgt_lang ja --src_emb wiki.en.vec --tgt_emb wiki.ja.vec --n_refinement 8 --n_epochs 10 --epoch_size 250000 --normalize_embeddings center --lr_shrink 0.75 --exp_name english_japanese
+#python supervised.py --src_lang en --tgt_lang ja --src_emb wiki.en.vec --tgt_emb wiki.ja.vec --n_refinement 8 --dico_train default --exp_name english_japanese_supervised
+
+# JA-EN debug
+python unsupervised.py --src_lang ja --tgt_lang en --src_emb wiki.ja.vec --tgt_emb wiki.en.vec --n_refinement 8 --n_epochs 12 --epoch_size 150000 --normalize_embeddings center --lr_shrink 0.75 --map_optimizer sgd,lr=0.2 --dis_optimizer sgd,lr=0.2 --exp_name japanese_debug
+
+# EN-JA 150000, 0.2 repro w/4242
+python unsupervised.py --src_lang en --tgt_lang ja --src_emb wiki.en.vec --tgt_emb wiki.ja.vec --n_refinement 8 --n_epochs 12 --epoch_size 150000 --normalize_embeddings center --lr_shrink 0.75 --map_optimizer sgd,lr=0.2 --dis_optimizer sgd,lr=0.2 --exp_name english_japanese_debug
+
+# JA-EN debug
+python unsupervised.py --src_lang ja --tgt_lang en --src_emb wiki.ja.vec --tgt_emb wiki.en.vec --n_refinement 8 --n_epochs 15 --epoch_size 100000 --normalize_embeddings center --lr_shrink 0.8 --map_optimizer sgd,lr=0.2 --dis_optimizer sgd,lr=0.2 --exp_name japanese_debug
+
diff --git a/src/dico_builder.py b/src/dico_builder.py
index 769ab9b..cf7ae4b 100644
--- a/src/dico_builder.py
+++ b/src/dico_builder.py
@@ -104,6 +104,10 @@ def get_candidates(emb1, emb2, params):
         all_targets[:, 0].unsqueeze(1)
     ], 1)
 
+    full_pairs = all_pairs
+
+    print(all_scores.size(), all_pairs.size(), n_src)
+
     # sanity check
     assert all_scores.size() == all_pairs.size() == (n_src, 2)
 
@@ -137,7 +141,7 @@ def get_candidates(emb1, emb2, params):
         mask = mask.unsqueeze(1).expand_as(all_pairs).clone()
         all_pairs = all_pairs.masked_select(mask).view(-1, 2)
 
-    return all_pairs
+    return all_pairs #, full_pairs
 
 
 def build_dictionary(src_emb, tgt_emb, params, s2t_candidates=None, t2s_candidates=None):
diff --git a/src/evaluation/__init__.py b/src/evaluation/__init__.py
index 8062096..003d1b5 100644
--- a/src/evaluation/__init__.py
+++ b/src/evaluation/__init__.py
@@ -1,4 +1,4 @@
 from .wordsim import get_wordsim_scores, get_crosslingual_wordsim_scores, get_wordanalogy_scores
-from .word_translation import get_word_translation_accuracy
+from .word_translation import get_word_translation_accuracy, discover_translations
 from .sent_translation import get_sent_translation_accuracy, load_europarl_data
 from .evaluator import Evaluator
diff --git a/src/evaluation/evaluator.py b/src/evaluation/evaluator.py
index 53106b6..94b332f 100644
--- a/src/evaluation/evaluator.py
+++ b/src/evaluation/evaluator.py
@@ -12,7 +12,7 @@ from torch.autograd import Variable
 from torch import Tensor as torch_tensor
 
 from . import get_wordsim_scores, get_crosslingual_wordsim_scores, get_wordanalogy_scores
-from . import get_word_translation_accuracy
+from . import get_word_translation_accuracy, discover_translations
 from . import load_europarl_data, get_sent_translation_accuracy
 from ..dico_builder import get_candidates, build_dictionary
 from src.utils import get_idf
@@ -104,7 +104,7 @@ class Evaluator(object):
         to_log['ws_crosslingual_scores'] = ws_crosslingual_scores
         to_log.update({'src_tgt_' + k: v for k, v in src_tgt_ws_scores.items()})
 
-    def word_translation(self, to_log):
+    def word_translation(self, to_log=None):
         """
         Evaluation on word translation.
         """
@@ -112,14 +112,12 @@ class Evaluator(object):
         src_emb = self.mapping(self.src_emb.weight).data
         tgt_emb = self.tgt_emb.weight.data
 
-        for method in ['nn', 'csls_knn_10']:
-            results = get_word_translation_accuracy(
-                self.src_dico.lang, self.src_dico.word2id, src_emb,
-                self.tgt_dico.lang, self.tgt_dico.word2id, tgt_emb,
-                method=method,
-                dico_eval=self.params.dico_eval
-            )
-            to_log.update([('%s-%s' % (k, method), v) for k, v in results])
+        results, matches = discover_translations(
+            self.src_dico.lang, self.src_dico.word2id, src_emb,
+            self.tgt_dico.lang, self.tgt_dico.word2id, tgt_emb,
+        )
+
+        return results, matches
 
     def sent_translation(self, to_log):
         """
diff --git a/src/evaluation/word_translation.py b/src/evaluation/word_translation.py
index 8f15640..85823fb 100644
--- a/src/evaluation/word_translation.py
+++ b/src/evaluation/word_translation.py
@@ -80,17 +80,68 @@ def load_dictionary(path, word2id1, word2id2):
     return dico
 
 
-def get_word_translation_accuracy(lang1, word2id1, emb1, lang2, word2id2, emb2, method, dico_eval):
+def discover_translations(lang1, word2id1, emb1, lang2, word2id2, emb2):
+
+    #start_idx = 0
+    #batch_size = 4500
+    #end_idx = start_idx + batch_size
+
+    src_word_ids = list(range(len(word2id1)))
+
+    # normalize word embeddings
+    emb1 = emb1 / emb1.norm(2, 1, keepdim=True).expand_as(emb1)
+    emb2 = emb2 / emb2.norm(2, 1, keepdim=True).expand_as(emb2)
+
+    output = np.zeros((len(src_word_ids), 5))
+    
+    #cont = True
+    #while cont:
+    #    if end_idx >= len(src_word_ids):
+    #        end_idx = len(src_word_ids)
+    #        cont = False
+
+    #    emb1_batch = emb1[start_idx:end_idx]
+
+    # average distances to k nearest neighbors
+    knn = 5
+    average_dist1 = get_nn_avg_dist(emb2, emb1, knn)
+    average_dist2 = get_nn_avg_dist(emb1, emb2, knn)
+    average_dist1 = torch.from_numpy(average_dist1).type_as(emb1)
+    average_dist2 = torch.from_numpy(average_dist2).type_as(emb2)
+    # queries / scores
+    query = emb1
+    scores = query.mm(emb2.transpose(0, 1))
+    scores.mul_(2)
+    scores.sub_(average_dist1[:, None])
+    scores.sub_(average_dist2[None, :])
+
+    #output[start_idx:end_idx] = scores.topk(5, 1, True)[1].int().cpu().numpy()
+    output = scores.topk(5, 1, True)[1].int().cpu().numpy()
+
+    #    start_idx += batch_size
+    #    end_idx += batch_size
+
+    return src_word_ids, output
+
+
+def get_word_translation_accuracy(lang1, word2id1, emb1, lang2, word2id2, emb2, method, dico_eval, loaded_dico=None):
     """
     Given source and target word embeddings, and a dictionary,
     evaluate the translation accuracy using the precision@k.
     """
-    if dico_eval == 'default':
-        path = os.path.join(DIC_EVAL_PATH, '%s-%s.5000-6500.txt' % (lang1, lang2))
+    if loaded_dico is None:
+        if dico_eval == 'default':
+            path = os.path.join(DIC_EVAL_PATH, '%s-%s.5000-6500.txt' % (lang1, lang2))
+        elif dico_eval == 'katakana':
+            path = os.path.join(DIC_EVAL_PATH, 'katakana_test_pairs.txt')
+        else:
+            path = dico_eval
+        dico = load_dictionary(path, word2id1, word2id2)
+        dico = dico.cuda() if emb1.is_cuda else dico
+        print(dico.size())
+        print(dico)
     else:
-        path = dico_eval
-    dico = load_dictionary(path, word2id1, word2id2)
-    dico = dico.cuda() if emb1.is_cuda else dico
+        dico = loaded_dico
 
     assert dico[:, 0].max() < emb1.size(0)
     assert dico[:, 1].max() < emb2.size(0)
diff --git a/src/models.py b/src/models.py
index 7464baf..9b33d8d 100644
--- a/src/models.py
+++ b/src/models.py
@@ -38,19 +38,36 @@ class Discriminator(nn.Module):
         return self.layers(x).view(-1)
 
 
-def build_model(params, with_dis):
+def update_src_embeddings(src_emb, params, idx):
+    # source embeddings
+    src_dico, _src_emb, src_freq_rank = load_embeddings(params, source=True, idx=idx)
+    params.src_dico = src_dico
+    if src_emb.num_embeddings != len(src_dico):
+        src_emb_new = nn.Embedding(len(src_dico), params.emb_dim, sparse=True)
+        src_emb = src_emb_new
+
+    src_emb.weight.data.copy_(_src_emb)
+
+    #if params.cuda:
+    #    src_emb.cuda()
+    
+    params.src_mean = normalize_embeddings(src_emb.weight.data, params.normalize_embeddings)
+
+    return src_freq_rank
+
+def build_model(params, with_dis, idx):
     """
     Build all components of the model.
     """
     # source embeddings
-    src_dico, _src_emb = load_embeddings(params, source=True)
+    src_dico, _src_emb, src_freq_rank = load_embeddings(params, source=True, idx=idx)
     params.src_dico = src_dico
     src_emb = nn.Embedding(len(src_dico), params.emb_dim, sparse=True)
     src_emb.weight.data.copy_(_src_emb)
 
     # target embeddings
     if params.tgt_lang:
-        tgt_dico, _tgt_emb = load_embeddings(params, source=False)
+        tgt_dico, _tgt_emb, _ = load_embeddings(params, source=False)
         params.tgt_dico = tgt_dico
         tgt_emb = nn.Embedding(len(tgt_dico), params.emb_dim, sparse=True)
         tgt_emb.weight.data.copy_(_tgt_emb)
@@ -79,4 +96,4 @@ def build_model(params, with_dis):
     if params.tgt_lang:
         params.tgt_mean = normalize_embeddings(tgt_emb.weight.data, params.normalize_embeddings)
 
-    return src_emb, tgt_emb, mapping, discriminator
+    return src_emb, tgt_emb, mapping, discriminator, src_freq_rank
diff --git a/src/utils.py b/src/utils.py
index 349c472..5f20fba 100644
--- a/src/utils.py
+++ b/src/utils.py
@@ -27,6 +27,8 @@ MAIN_DUMP_PATH = os.path.join(os.path.dirname(os.path.dirname(os.path.realpath(_
 
 logger = getLogger()
 
+bad_words = ['moreover', 'and', 'to', 'the', 'a', 'however', 'that', 'of', 'furthermore', 'hence', 'which', 'where', 'what', 'how', 'why', 'when', 'or', 'else', 'in', 'an', 'for', 'likewise', 'instance', 'nevertheless', 'but', 'thus', 'therefore', 'lastly', 'consequentially', 'even', 'though', 'although', 'at', 'then', 'while', 'finally', 'besides', 'especially', 'both', 'because', 'consequently', 'importantly', 'indeed', 'nonetheless', 'particular', 'well', 'forementioned', 'addition', 'additionally', 'particularly', 'these', 'this', 'extent', 'by', 'crucially', 'similarly']
+
 
 # load Faiss if available (dramatically accelerates the nearest neighbor search)
 try:
@@ -261,18 +263,27 @@ def clip_parameters(model, clip):
             x.data.clamp_(-clip, clip)
 
 
-def read_txt_embeddings(params, source, full_vocab):
+def read_txt_embeddings(params, source, full_vocab, idx=1):
     """
     Reload pretrained embeddings from a text file.
     """
     word2id = {}
+    word2freqRank = {}
     vectors = []
 
     # load pretrained embeddings
     lang = params.src_lang if source else params.tgt_lang
+
+    if lang == 'ja':
+        params.max_vocab = 4500
+
+    if lang == 'en':
+        params.max_vocab = 500000
+
     emb_path = params.src_emb if source else params.tgt_emb
     _emb_dim_file = params.emb_dim
     with io.open(emb_path, 'r', encoding='utf-8', newline='\n', errors='ignore') as f:
+        cntr = 1
         for i, line in enumerate(f):
             if i == 0:
                 split = line.split()
@@ -280,8 +291,37 @@ def read_txt_embeddings(params, source, full_vocab):
                 assert _emb_dim_file == int(split[1])
             else:
                 word, vect = line.rstrip().split(' ', 1)
+
                 if not full_vocab:
                     word = word.lower()
+
+                if lang == 'en':
+                    skip = False
+                    for c in word:
+                        if not(ord(c) >= 0x0041 and ord(c) <= 0x005A) and not(ord(c) >= 0x0061 and ord(c) <= 0x007A):
+                            skip = True
+                            break
+
+                        if word in bad_words:
+                            skip = True
+
+                    if skip:
+                        continue
+
+                if lang == 'ja':
+                    skip = False
+                    for c in word:
+                        if not(ord(c) >= 0x30A0 and ord(c) <= 0x30FF):
+                            skip = True
+                            break
+
+                    if skip:
+                        continue
+        
+                    if cntr < idx:
+                        cntr += 1
+                        continue
+
                 vect = np.fromstring(vect, sep=' ')
                 if np.linalg.norm(vect) == 0:  # avoid to have null embeddings
                     vect[0] = 0.01
@@ -296,12 +336,14 @@ def read_txt_embeddings(params, source, full_vocab):
                         continue
                     assert vect.shape == (_emb_dim_file,), i
                     word2id[word] = len(word2id)
+                    word2freqRank[word] = i - 1
                     vectors.append(vect[None])
             if params.max_vocab > 0 and len(word2id) >= params.max_vocab and not full_vocab:
                 break
 
     assert len(word2id) == len(vectors)
-    logger.info("Loaded %i pre-trained word embeddings." % len(vectors))
+    if idx == 1 or lang == 'en':
+        logger.info("Loaded %i pre-trained word embeddings." % len(vectors))
 
     # compute new vocabulary / embeddings
     id2word = {v: k for k, v in word2id.items()}
@@ -310,8 +352,12 @@ def read_txt_embeddings(params, source, full_vocab):
     embeddings = torch.from_numpy(embeddings).float()
     embeddings = embeddings.cuda() if (params.cuda and not full_vocab) else embeddings
 
+    if lang == 'ja':
+        params.max_vocab = -1
+
     assert embeddings.size() == (len(dico), params.emb_dim)
-    return dico, embeddings
+
+    return dico, embeddings, word2freqRank
 
 
 def select_subset(word_list, max_vocab):
@@ -384,7 +430,7 @@ def load_bin_embeddings(params, source, full_vocab):
     return dico, embeddings
 
 
-def load_embeddings(params, source, full_vocab=False):
+def load_embeddings(params, source, idx=-1, full_vocab=False):
     """
     Reload pretrained embeddings.
     - `full_vocab == False` means that we load the `params.max_vocab` most frequent words.
@@ -403,7 +449,7 @@ def load_embeddings(params, source, full_vocab=False):
     if emb_path.endswith('.bin'):
         return load_bin_embeddings(params, source, full_vocab)
     else:
-        return read_txt_embeddings(params, source, full_vocab)
+        return read_txt_embeddings(params, source, full_vocab, idx=idx)
 
 
 def normalize_embeddings(emb, types, mean=None):
diff --git a/supervised.py b/supervised.py
index 3675082..f76d4f3 100644
--- a/supervised.py
+++ b/supervised.py
@@ -33,7 +33,7 @@ parser.add_argument("--export", type=str, default="txt", help="Export embeddings
 
 # data
 parser.add_argument("--src_lang", type=str, default='en', help="Source language")
-parser.add_argument("--tgt_lang", type=str, default='es', help="Target language")
+parser.add_argument("--tgt_lang", type=str, default='ja', help="Target language")
 parser.add_argument("--emb_dim", type=int, default=300, help="Embedding dimension")
 parser.add_argument("--max_vocab", type=int, default=200000, help="Maximum vocabulary size (-1 to disable)")
 # training refinement
@@ -64,7 +64,7 @@ assert params.dico_max_size == 0 or params.dico_max_size < params.dico_max_rank
 assert params.dico_max_size == 0 or params.dico_max_size > params.dico_min_size
 assert os.path.isfile(params.src_emb)
 assert os.path.isfile(params.tgt_emb)
-assert params.dico_eval == 'default' or os.path.isfile(params.dico_eval)
+#assert params.dico_eval == 'default' or os.path.isfile(params.dico_eval)
 assert params.export in ["", "txt", "pth"]
 
 # build logger / model / trainer / evaluator
diff --git a/unsupervised.py b/unsupervised.py
index 1c8d9cd..f373f2d 100644
--- a/unsupervised.py
+++ b/unsupervised.py
@@ -24,7 +24,7 @@ VALIDATION_METRIC = 'mean_cosine-csls_knn_10-S2T-10000'
 
 # main
 parser = argparse.ArgumentParser(description='Unsupervised training')
-parser.add_argument("--seed", type=int, default=-1, help="Initialization seed")
+parser.add_argument("--seed", type=int, default=4242, help="Initialization seed")
 parser.add_argument("--verbose", type=int, default=2, help="Verbose level (2:debug, 1:info, 0:warning)")
 parser.add_argument("--exp_path", type=str, default="", help="Where to store experiment logs and models")
 parser.add_argument("--exp_name", type=str, default="debug", help="Experiment name")
@@ -33,7 +33,7 @@ parser.add_argument("--cuda", type=bool_flag, default=True, help="Run on GPU")
 parser.add_argument("--export", type=str, default="txt", help="Export embeddings after training (txt / pth)")
 # data
 parser.add_argument("--src_lang", type=str, default='en', help="Source language")
-parser.add_argument("--tgt_lang", type=str, default='es', help="Target language")
+parser.add_argument("--tgt_lang", type=str, default='ja', help="Target language")
 parser.add_argument("--emb_dim", type=int, default=300, help="Embedding dimension")
 parser.add_argument("--max_vocab", type=int, default=200000, help="Maximum vocabulary size (-1 to disable)")
 # mapping
@@ -54,11 +54,11 @@ parser.add_argument("--adversarial", type=bool_flag, default=True, help="Use adv
 parser.add_argument("--n_epochs", type=int, default=5, help="Number of epochs")
 parser.add_argument("--epoch_size", type=int, default=1000000, help="Iterations per epoch")
 parser.add_argument("--batch_size", type=int, default=32, help="Batch size")
-parser.add_argument("--map_optimizer", type=str, default="sgd,lr=0.1", help="Mapping optimizer")
-parser.add_argument("--dis_optimizer", type=str, default="sgd,lr=0.1", help="Discriminator optimizer")
-parser.add_argument("--lr_decay", type=float, default=0.98, help="Learning rate decay (SGD only)")
+parser.add_argument("--map_optimizer", type=str, default="sgd,lr=0.2", help="Mapping optimizer")
+parser.add_argument("--dis_optimizer", type=str, default="sgd,lr=0.2", help="Discriminator optimizer")
+parser.add_argument("--lr_decay", type=float, default=0.95, help="Learning rate decay (SGD only)")
 parser.add_argument("--min_lr", type=float, default=1e-6, help="Minimum learning rate (SGD only)")
-parser.add_argument("--lr_shrink", type=float, default=0.5, help="Shrink the learning rate if the validation metric decreases (1 to disable)")
+parser.add_argument("--lr_shrink", type=float, default=1, help="Shrink the learning rate if the validation metric decreases (1 to disable)")
 # training refinement
 parser.add_argument("--n_refinement", type=int, default=5, help="Number of refinement iterations (0 to disable the refinement procedure)")
 # dictionary creation parameters (for refinement)
@@ -70,8 +70,8 @@ parser.add_argument("--dico_max_rank", type=int, default=15000, help="Maximum di
 parser.add_argument("--dico_min_size", type=int, default=0, help="Minimum generated dictionary size (0 to disable)")
 parser.add_argument("--dico_max_size", type=int, default=0, help="Maximum generated dictionary size (0 to disable)")
 # reload pre-trained embeddings
-parser.add_argument("--src_emb", type=str, default="", help="Reload source embeddings")
-parser.add_argument("--tgt_emb", type=str, default="", help="Reload target embeddings")
+parser.add_argument("--src_emb", type=str, default="wiki.en.vec", help="Reload source embeddings")
+parser.add_argument("--tgt_emb", type=str, default="wiki.ja.vec", help="Reload target embeddings")
 parser.add_argument("--normalize_embeddings", type=str, default="", help="Normalize embeddings before training")
 
 
@@ -87,16 +87,16 @@ assert params.dis_lambda > 0 and params.dis_steps > 0
 assert 0 < params.lr_shrink <= 1
 assert os.path.isfile(params.src_emb)
 assert os.path.isfile(params.tgt_emb)
-assert params.dico_eval == 'default' or os.path.isfile(params.dico_eval)
+#assert params.dico_eval == 'default' or os.path.isfile(params.dico_eval)
 assert params.export in ["", "txt", "pth"]
 
 # build model / trainer / evaluator
 logger = initialize_exp(params)
+
 src_emb, tgt_emb, mapping, discriminator = build_model(params, True)
 trainer = Trainer(src_emb, tgt_emb, mapping, discriminator, params)
 evaluator = Evaluator(trainer)
 
-
 """
 Learning loop for Adversarial Training
 """
@@ -176,6 +176,10 @@ if params.n_refinement > 0:
 
         # JSON log / save best model / end of epoch
         logger.info("__log__:%s" % json.dumps(to_log))
+
+        if n_iter == 0:
+            trainer.best_valid_metric = to_log[VALIDATION_METRIC] - 0.01
+
         trainer.save_best(to_log, VALIDATION_METRIC)
         logger.info('End of refinement iteration %i.\n\n' % n_iter)
 
