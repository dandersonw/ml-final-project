{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "tf.enable_eager_execution()\n",
    "import importlib\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "from transliteration import data, train, model_one, script, decode, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "importlib.reload(data)\n",
    "batch_size = 128\n",
    "cmu_train_dataset = data.make_dataset('../data/tfrecord/cmu_train.tfrecord',\n",
    "                                  from_script='en',\n",
    "                                  to_script='cmu',\n",
    "                                  batch_size=batch_size)\n",
    "cmu_valid_dataset = data.make_dataset('../data/tfrecord/cmu_valid.tfrecord',\n",
    "                                  from_script='en',\n",
    "                                  to_script='cmu',\n",
    "                                  batch_size=batch_size)\n",
    "cmu_test_dataset = data.make_dataset('../data/tfrecord/cmu_test.tfrecord',\n",
    "                                 from_script='en',\n",
    "                                 to_script='cmu',\n",
    "                                 batch_size=batch_size)\n",
    "eob_train_dataset = data.make_dataset('../data/tfrecord/eob_train.tfrecord',\n",
    "                                       from_script='en',\n",
    "                                       to_script='ja',\n",
    "                                       batch_size=batch_size)\n",
    "eob_valid_dataset = data.make_dataset('../data/tfrecord/eob_valid.tfrecord',\n",
    "                                       from_script='en',\n",
    "                                       to_script='ja',\n",
    "                                       batch_size=batch_size)\n",
    "eob_test_dataset = data.make_dataset('../data/tfrecord/eob_test.tfrecord',\n",
    "                                      from_script='en',\n",
    "                                      to_script='ja',\n",
    "                                      batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred)\n",
    "    return tf.reduce_mean(loss_ * mask)\n",
    "\n",
    "cmu_encoder_config = model_one.Config(lstm_size=240,\n",
    "                                      embedding_size=30,\n",
    "                                      attention_size=None,\n",
    "                                      vocab_size=script.SCRIPTS['en'].vocab_size)\n",
    "cmu_decoder_config = model_one.Config(lstm_size=240,\n",
    "                                      embedding_size=30,\n",
    "                                      attention_size=120,\n",
    "                                      attention='monotonic_bahdanau',\n",
    "                                      vocab_size=script.SCRIPTS['cmu'].vocab_size)\n",
    "ja_decoder_config = model_one.Config(lstm_size=240,\n",
    "                                     embedding_size=30,\n",
    "                                     attention_size=120,\n",
    "                                     attention='multiple:monotonic_bahdanau,monotonic_bahdanau',\n",
    "                                     vocab_size=script.SCRIPTS['ja'].vocab_size)\n",
    "cmu_encoder = model_one.Encoder(cmu_encoder_config)\n",
    "cmu_decoder = model_one.Decoder(cmu_decoder_config)\n",
    "ja_encoder = model_one.StackedEncoderDecoderEncoder(cmu_encoder, cmu_decoder, 'cmu')\n",
    "ja_decoder = model_one.Decoder(ja_decoder_config)\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 cmu_encoder=cmu_encoder,\n",
    "                                 ja_decoder=ja_decoder,\n",
    "                                 cmu_decoder=cmu_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/derick/anaconda3/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss 15.451, Valid Loss 8.827\n",
      "([['D EH1 R K IH0 K ER0 Z', 'D EH1 R K W EH2 K ER0 Z']], array([[-6.8596259 , -8.28446002]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss 6.500, Valid Loss 5.189\n",
      "([['D ER1 K IH0 K', 'D EH1 R CH IH0 K']], array([[-3.74216951, -3.85924087]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss 4.291, Valid Loss 3.858\n",
      "([['D EH1 R IH0 K', 'D ER1 IH0 K']], array([[-2.75024256, -2.84142637]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss 3.260, Valid Loss 3.151\n",
      "([['D ER1 IH0 K', 'D EH1 R IH0 K']], array([[-2.53842798, -2.94311621]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss 2.629, Valid Loss 2.674\n",
      "([['D ER1 IH0 K', 'D EH1 R IH0 K']], array([[-1.57642895, -1.91252233]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss 2.209, Valid Loss 2.397\n",
      "([['D EH1 R IH0 K T', 'D ER0 IH1 K T']], array([[-1.66566963, -2.03239874]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss 1.933, Valid Loss 2.263\n",
      "([['D EH1 R IH0 K T ER0', 'D EH1 R IH0 K T']], array([[-1.96778044, -2.28633122]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss 1.740, Valid Loss 2.184\n",
      "([['D EH1 R IH0 K T', 'D EH1 R IH0 K S']], array([[-2.44807088, -2.75266555]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss 1.582, Valid Loss 2.146\n",
      "([['D EH1 R IH0 K AH0 L T', 'D EH1 R IH0 K IH0 V']], array([[-1.64011403, -2.67278628]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss 1.448, Valid Loss 2.126\n",
      "([['D EH1 R IH0 K AH0 N', 'D EH1 R IH0 K AH0 L']], array([[-2.50181594, -2.5455018 ]]))\n"
     ]
    }
   ],
   "source": [
    "cmu_best_val_loss = None\n",
    "cmu_checkpoint = None\n",
    "for e in range(10):\n",
    "    loss = train.run_one_epoch(cmu_train_dataset,\n",
    "                               True,\n",
    "                               from_script='en',\n",
    "                               to_script='cmu',\n",
    "                               encoder=cmu_encoder,\n",
    "                               decoder=cmu_decoder,\n",
    "                               optimizer=optimizer,\n",
    "                               loss_function=loss_function)\n",
    "    valid_loss = train.run_one_epoch(cmu_valid_dataset,\n",
    "                                     False,\n",
    "                                     from_script='en',\n",
    "                                     to_script='cmu',\n",
    "                                     encoder=cmu_encoder,\n",
    "                                     decoder=cmu_decoder,\n",
    "                                     loss_function=loss_function)\n",
    "    if cmu_best_val_loss is None or valid_loss < cmu_best_val_loss:\n",
    "        cmu_best_val_loss = valid_loss\n",
    "        cmu_checkpoint = checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    else:\n",
    "        break\n",
    "    print(\"Epoch {}: Train Loss {:.3f}, Valid Loss {:.3f}\".format(e, loss, valid_loss))\n",
    "    print(decode.transliterate(input_strs=['derick'],\n",
    "                               from_script='en',\n",
    "                               to_script='cmu',\n",
    "                               encoder=cmu_encoder,\n",
    "                               decoder=cmu_decoder,\n",
    "                               k_best=2,\n",
    "                               decoding_method=decode.beam_search_decode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.1078532, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "checkpoint.restore(cmu_checkpoint).assert_consumed()\n",
    "print(train.run_one_epoch(cmu_valid_dataset,\n",
    "                          False,\n",
    "                          from_script='en',\n",
    "                          to_script='cmu',\n",
    "                          encoder=cmu_encoder,\n",
    "                          decoder=cmu_decoder,\n",
    "                          loss_function=loss_function))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def run_some_epochs(epochs):\n",
    "    checkpoint_path = None\n",
    "    best_val_loss = None\n",
    "    for e in range(epochs):\n",
    "        loss = train.run_one_epoch(eob_train_dataset,\n",
    "                                   True,\n",
    "                                   from_script='en',\n",
    "                                   to_script='ja',\n",
    "                                   encoder=ja_encoder,\n",
    "                                   decoder=ja_decoder,\n",
    "                                   optimizer=optimizer,\n",
    "                                   loss_function=loss_function)\n",
    "        valid_loss = train.run_one_epoch(eob_valid_dataset,\n",
    "                                         False,\n",
    "                                         from_script='en',\n",
    "                                         to_script='ja',\n",
    "                                         encoder=ja_encoder,\n",
    "                                         decoder=ja_decoder,\n",
    "                                         loss_function=loss_function)\n",
    "        print(\"Epoch {}: Train Loss {:.3f}, Valid Loss {:.3f}\".format(e, loss, valid_loss))\n",
    "        print(decode.transliterate(input_strs=['derick'],\n",
    "                                       from_script='en',\n",
    "                                       to_script='ja',\n",
    "                                       encoder=ja_encoder,\n",
    "                                       decoder=ja_decoder,\n",
    "                                       k_best=2,\n",
    "                                       decoding_method=decode.beam_search_decode))\n",
    "        if best_val_loss is None or valid_loss < best_val_loss:\n",
    "            best_val_loss = valid_loss\n",
    "            checkpoint_path = checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "        else:\n",
    "            break\n",
    "    return checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss 12.920, Valid Loss 7.402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['ディリク', 'ディリック']], array([[-4.04340054, -4.97185265]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss 6.220, Valid Loss 5.784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['デリック', 'ディリック']], array([[-2.83951345, -4.08381903]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss 4.678, Valid Loss 5.330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['デリック', 'ディリック']], array([[-2.64089123, -4.6104294 ]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss 3.754, Valid Loss 5.169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['デリック', 'ダリック']], array([[-2.58163609, -3.48164397]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss 3.063, Valid Loss 5.032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['デリック', 'デリ']], array([[-2.65950389, -2.77438992]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss 2.493, Valid Loss 5.030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['デリカック', 'ディリック']], array([[-3.54263781, -3.6919361 ]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss 2.022, Valid Loss 5.180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['デリ', 'デリカック']], array([[-2.51001856, -2.77032691]]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=307138649, shape=(), dtype=float32, numpy=5.048813>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for layer in cmu_encoder.layers:\n",
    "    layer.trainable = False\n",
    "for layer in cmu_decoder.layers:\n",
    "    layer.trainable = False\n",
    "checkpoint_path = run_some_epochs(10)\n",
    "checkpoint.restore(checkpoint_path).assert_consumed()\n",
    "train.run_one_epoch(eob_valid_dataset,\n",
    "                    False,\n",
    "                    from_script='en',\n",
    "                    to_script='ja',\n",
    "                    encoder=ja_encoder,\n",
    "                    decoder=ja_decoder,\n",
    "                    loss_function=loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "valid_df = pd.read_csv('../data/split/eob_pairs_valid.csv',\n",
    "                       keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4375684556407448"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = decode.transliterate(input_strs=valid_df['en'].values,\n",
    "                     from_script='en',\n",
    "                     to_script='ja',\n",
    "                     encoder=ja_encoder,\n",
    "                     decoder=ja_decoder,\n",
    "                     k_best=10,\n",
    "                     num_beams=20,\n",
    "                     decoding_method=decode.beam_search_decode)\n",
    "evaluate.top_k_accuracy(valid_df['ja'].values, tr, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "valid_df = pd.read_csv('../data/split/muse_pairs_valid.csv',\n",
    "                       keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3066485753052917"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = decode.transliterate(input_strs=valid_df['en'].values,\n",
    "                     from_script='en',\n",
    "                     to_script='ja',\n",
    "                     encoder=ja_encoder,\n",
    "                     decoder=ja_decoder,\n",
    "                     k_best=10,\n",
    "                     num_beams=20,\n",
    "                     decoding_method=decode.beam_search_decode)\n",
    "evaluate.top_k_accuracy(valid_df['ja'].values, tr, k=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-1.13]",
   "name": "conda-env-tf-1.13-py"
  },
  "name": "stacked_transfer.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
