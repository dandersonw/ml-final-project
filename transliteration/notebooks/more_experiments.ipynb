{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "tf.enable_eager_execution()\n",
    "import importlib\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "from transliteration import data, train, model_one, script, decode, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/derick/anaconda3/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(data)\n",
    "batch_size = 128\n",
    "cmu_train_dataset = data.make_dataset('../data/tfrecord/cmu_train.tfrecord',\n",
    "                                      from_script='en',\n",
    "                                      to_script='cmu',\n",
    "                                      combine_words_proportion=0.3,\n",
    "                                      batch_size=batch_size)\n",
    "cmu_valid_dataset = data.make_dataset('../data/tfrecord/cmu_valid.tfrecord',\n",
    "                                      from_script='en',\n",
    "                                      to_script='cmu',\n",
    "                                      combine_words_proportion=0.3,\n",
    "                                      batch_size=batch_size)\n",
    "cmu_test_dataset = data.make_dataset('../data/tfrecord/cmu_test.tfrecord',\n",
    "                                     from_script='en',\n",
    "                                     to_script='cmu',\n",
    "                                     combine_words_proportion=0.3,\n",
    "                                     batch_size=batch_size)\n",
    "eob_train_dataset = data.make_dataset('../data/tfrecord/eob_train.tfrecord',\n",
    "                                      from_script='en',\n",
    "                                      to_script='ja',\n",
    "                                      batch_size=batch_size)\n",
    "eob_valid_dataset = data.make_dataset('../data/tfrecord/eob_valid.tfrecord',\n",
    "                                      from_script='en',\n",
    "                                      to_script='ja',\n",
    "                                      batch_size=batch_size)\n",
    "eob_test_dataset = data.make_dataset('../data/tfrecord/eob_test.tfrecord',\n",
    "                                     from_script='en',\n",
    "                                     to_script='ja',\n",
    "                                     batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred)\n",
    "    return tf.reduce_mean(loss_ * mask)\n",
    "\n",
    "encoder_config = model_one.Config(lstm_size=240,\n",
    "                                  embedding_size=30,\n",
    "                                  attention_size=None,\n",
    "                                  vocab_size=script.SCRIPTS['en'].vocab_size)\n",
    "ja_decoder_config = model_one.Config(lstm_size=240,\n",
    "                                     embedding_size=30,\n",
    "                                     attention_size=120,\n",
    "                                     attention='monotonic_bahdanau',\n",
    "                                     vocab_size=script.SCRIPTS['ja'].vocab_size)\n",
    "cmu_decoder_config = model_one.Config(lstm_size=240,\n",
    "                                      embedding_size=30,\n",
    "                                      attention_size=120,\n",
    "                                      attention='monotonic_bahdanau',\n",
    "                                      vocab_size=script.SCRIPTS['cmu'].vocab_size)\n",
    "encoder = model_one.Encoder(encoder_config)\n",
    "ja_decoder = model_one.Decoder(ja_decoder_config)\n",
    "cmu_decoder = model_one.Decoder(cmu_decoder_config)\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 ja_decoder=ja_decoder,\n",
    "                                 cmu_decoder=cmu_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss 24.251, Valid Loss 14.589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['D R IH1 K K K IH0 NG K', 'D R IH1 K K K IH0 NG K AH0 N']], array([[-13.59901275, -17.61309446]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss 10.364, Valid Loss 7.040\n",
      "([['D EH1 R IH0 K IH0 NG', 'D EH1 R IH0 K IH0 K S ER0']], array([[-5.80079673, -9.81030743]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss 5.727, Valid Loss 4.854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['D EH1 R IH0 K IH0 NG', 'D EH1 R IH0 K EH2 L IH0 NG']], array([[-5.13794947, -8.80927134]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss 4.252, Valid Loss 3.993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['D EH1 R IH0 K ER0 Z', 'D EH1 R IH0 K AH0 L AY2 Z']], array([[-5.24770398, -8.08406002]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss 3.593, Valid Loss 3.459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['D EH1 R IH0 K ER0 Z', 'D EH1 R IH0 K AO2 R Z']], array([[-4.54251916, -6.78896037]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss 3.207, Valid Loss 3.183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['D EH1 R IH0 K', 'D EH1 R IH0 K ER0']], array([[-3.11693265, -3.53123404]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss 2.956, Valid Loss 3.095\n",
      "([['D ER0 IH1 K ER0 Z', 'D EH1 R IH0 K']], array([[-3.35321811, -3.66339079]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss 2.771, Valid Loss 2.971\n",
      "([['D EH1 R IH0 K ER0 Z', 'D EH1 R IH0 K AH0 N']], array([[-2.90840366, -4.89875054]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss 2.606, Valid Loss 2.798\n",
      "([['D EH1 R IH0 K ER0 Z', 'D ER0 IH1 K ER0 Z']], array([[-3.56117671, -3.64637422]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss 2.457, Valid Loss 2.748\n",
      "([['D EH1 R IH0 K', 'D EH1 R IH0 K ER0 Z']], array([[-2.47603393, -3.53397996]]))\n"
     ]
    }
   ],
   "source": [
    "cmu_best_val_loss = None\n",
    "cmu_checkpoint = None\n",
    "for e in range(10):\n",
    "    loss = train.run_one_epoch(cmu_train_dataset,\n",
    "                               True,\n",
    "                               from_script='en',\n",
    "                               to_script='cmu',\n",
    "                               encoder=encoder,\n",
    "                               decoder=cmu_decoder,\n",
    "                               optimizer=optimizer,\n",
    "                               loss_function=loss_function)\n",
    "    valid_loss = train.run_one_epoch(cmu_valid_dataset,\n",
    "                                     False,\n",
    "                                     from_script='en',\n",
    "                                     to_script='cmu',\n",
    "                                     encoder=encoder,\n",
    "                                     decoder=cmu_decoder,\n",
    "                                     loss_function=loss_function)\n",
    "    if cmu_best_val_loss is None or valid_loss < cmu_best_val_loss:\n",
    "        cmu_best_val_loss = valid_loss\n",
    "        cmu_checkpoint = checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    print(\"Epoch {}: Train Loss {:.3f}, Valid Loss {:.3f}\".format(e, loss, valid_loss))\n",
    "    print(decode.transliterate(input_strs=['derick'],\n",
    "                               from_script='en',\n",
    "                               to_script='cmu',\n",
    "                               encoder=encoder,\n",
    "                               decoder=cmu_decoder,\n",
    "                               k_best=2,\n",
    "                               decoding_method=decode.beam_search_decode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.802323, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "checkpoint.restore(cmu_checkpoint).assert_consumed()\n",
    "print(train.run_one_epoch(cmu_valid_dataset,\n",
    "                          False,\n",
    "                          from_script='en',\n",
    "                          to_script='cmu',\n",
    "                          encoder=encoder,\n",
    "                          decoder=cmu_decoder,\n",
    "                          loss_function=loss_function))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "encoder.save_weights('./training_checkpoints/encoder_cmu_only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def run_some_epochs(epochs):\n",
    "    checkpoint_path = None\n",
    "    best_val_loss = None\n",
    "    for e in range(epochs):\n",
    "        loss = train.run_one_epoch(eob_train_dataset,\n",
    "                                   True,\n",
    "                                   from_script='en',\n",
    "                                   to_script='ja',\n",
    "                                   encoder=encoder,\n",
    "                                   decoder=ja_decoder,\n",
    "                                   optimizer=optimizer,\n",
    "                                   loss_function=loss_function)\n",
    "        valid_loss = train.run_one_epoch(eob_valid_dataset,\n",
    "                                         False,\n",
    "                                         from_script='en',\n",
    "                                         to_script='ja',\n",
    "                                         encoder=encoder,\n",
    "                                         decoder=ja_decoder,\n",
    "                                         loss_function=loss_function)\n",
    "        print(\"Epoch {}: Train Loss {:.3f}, Valid Loss {:.3f}\".format(e, loss, valid_loss))\n",
    "        print(decode.transliterate(input_strs=['derick'],\n",
    "                                       from_script='en',\n",
    "                                       to_script='ja',\n",
    "                                       encoder=encoder,\n",
    "                                       decoder=ja_decoder,\n",
    "                                       k_best=2,\n",
    "                                       decoding_method=decode.beam_search_decode))\n",
    "        if best_val_loss is None or valid_loss < best_val_loss:\n",
    "            best_val_loss = valid_loss\n",
    "            checkpoint_path = checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    return checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss 15.029, Valid Loss 7.992\n",
      "([['デリック', 'ディック']], array([[-4.20449633, -4.48618308]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss 6.801, Valid Loss 5.887\n",
      "([['デリック', 'ディック']], array([[-3.39015117, -4.17438946]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss 4.986, Valid Loss 5.087\n",
      "([['デリック', 'ディック']], array([[-2.57261912, -3.45445298]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss 3.980, Valid Loss 4.891\n",
      "([['デリック', 'ディック']], array([[-2.34256739, -4.12991123]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss 3.264, Valid Loss 4.785\n",
      "([['デリック', 'デリ']], array([[-1.92635161, -3.25199075]]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=174943845, shape=(), dtype=float32, numpy=4.789574>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for layer in encoder.layers:\n",
    "    layer.trainable = False\n",
    "checkpoint_path = run_some_epochs(5)\n",
    "checkpoint.restore(checkpoint_path).assert_consumed()\n",
    "train.run_one_epoch(eob_valid_dataset,\n",
    "                    False,\n",
    "                    from_script='en',\n",
    "                    to_script='ja',\n",
    "                    encoder=encoder,\n",
    "                    decoder=ja_decoder,\n",
    "                    loss_function=loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss 2.738, Valid Loss 4.695\n",
      "([['デリック', 'デリ']], array([[-1.98842829, -3.12527804]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss 2.272, Valid Loss 4.783\n",
      "([['デリック', 'デリックス']], array([[-1.69626716, -3.9795914 ]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss 1.897, Valid Loss 4.803\n",
      "([['デリック', 'ダリック']], array([[-1.29844523, -3.14170392]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss 1.604, Valid Loss 4.952\n",
      "([['デリック', 'ディリック']], array([[-1.45784592, -2.14429108]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss 1.316, Valid Loss 4.973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['デリック', 'デリックス']], array([[-1.11313215, -3.10429392]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss 1.104, Valid Loss 5.155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['デリック', 'ディック']], array([[-1.17609765, -3.13347937]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss 0.933, Valid Loss 5.537\n",
      "([['デリック', 'ディリック']], array([[-0.92497172, -2.38745292]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss 0.797, Valid Loss 5.518\n",
      "([['デリック', 'ディック']], array([[-1.22925424, -1.90467167]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss 0.691, Valid Loss 5.655\n",
      "([['ディリック', 'デリック']], array([[-0.86461503, -2.02275647]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss 0.594, Valid Loss 5.645\n",
      "([['デリック', 'ディリック']], array([[-1.01886313, -1.67887097]]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=219422862, shape=(), dtype=float32, numpy=4.6767993>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for layer in encoder.layers:\n",
    "    layer.trainable = True\n",
    "checkpoint_path = run_some_epochs(10)\n",
    "checkpoint.restore(checkpoint_path).assert_consumed()\n",
    "train.run_one_epoch(eob_valid_dataset,\n",
    "                    False,\n",
    "                    from_script='en',\n",
    "                    to_script='ja',\n",
    "                    encoder=encoder,\n",
    "                    decoder=ja_decoder,\n",
    "                    loss_function=loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "valid_df = pd.read_csv('../data/split/eob_pairs_valid.csv',\n",
    "                       keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5180722891566265"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = decode.transliterate(input_strs=valid_df['en'].values,\n",
    "                     from_script='en',\n",
    "                     to_script='ja',\n",
    "                     encoder=encoder,\n",
    "                     decoder=ja_decoder,\n",
    "                     k_best=10,\n",
    "                     num_beams=20,\n",
    "                     decoding_method=decode.beam_search_decode)\n",
    "evaluate.top_k_accuracy(valid_df['ja'].values, tr, k=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-1.13]",
   "name": "conda-env-tf-1.13-py"
  },
  "name": "more_experiments.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
