{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "tf.enable_eager_execution()\n",
    "import importlib\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "from transliteration import data, train, model_one, script, decode, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "importlib.reload(data)\n",
    "batch_size = 128\n",
    "cmu_train_dataset = data.make_dataset('../data/tfrecord/cmu_train.tfrecord',\n",
    "                                  from_script='en',\n",
    "                                  to_script='cmu',\n",
    "                                  batch_size=batch_size)\n",
    "cmu_valid_dataset = data.make_dataset('../data/tfrecord/cmu_valid.tfrecord',\n",
    "                                  from_script='en',\n",
    "                                  to_script='cmu',\n",
    "                                  batch_size=batch_size)\n",
    "cmu_test_dataset = data.make_dataset('../data/tfrecord/cmu_test.tfrecord',\n",
    "                                 from_script='en',\n",
    "                                 to_script='cmu',\n",
    "                                 batch_size=batch_size)\n",
    "eob_train_dataset = data.make_dataset('../data/tfrecord/eob_train.tfrecord',\n",
    "                                       from_script='en',\n",
    "                                       to_script='ja',\n",
    "                                       batch_size=batch_size)\n",
    "eob_valid_dataset = data.make_dataset('../data/tfrecord/eob_valid.tfrecord',\n",
    "                                       from_script='en',\n",
    "                                       to_script='ja',\n",
    "                                       batch_size=batch_size)\n",
    "eob_test_dataset = data.make_dataset('../data/tfrecord/eob_test.tfrecord',\n",
    "                                      from_script='en',\n",
    "                                      to_script='ja',\n",
    "                                      batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred)\n",
    "    return tf.reduce_mean(loss_ * mask)\n",
    "\n",
    "encoder_config = model_one.Config(lstm_size=240,\n",
    "                                  embedding_size=30,\n",
    "                                  attention_size=None,\n",
    "                                  vocab_size=script.SCRIPTS['en'].vocab_size)\n",
    "ja_decoder_config = model_one.Config(lstm_size=240,\n",
    "                                     embedding_size=30,\n",
    "                                     attention_size=120,\n",
    "                                     attention='monotonic_bahdanau',\n",
    "                                     vocab_size=script.SCRIPTS['ja'].vocab_size)\n",
    "cmu_decoder_config = model_one.Config(lstm_size=240,\n",
    "                                      embedding_size=30,\n",
    "                                      attention_size=120,\n",
    "                                      attention='monotonic_bahdanau',\n",
    "                                      vocab_size=script.SCRIPTS['cmu'].vocab_size)\n",
    "encoder = model_one.Encoder(encoder_config)\n",
    "ja_decoder = model_one.Decoder(ja_decoder_config)\n",
    "cmu_decoder = model_one.Decoder(cmu_decoder_config)\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 ja_decoder=ja_decoder,\n",
    "                                 cmu_decoder=cmu_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/derick/anaconda3/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss 10.731, Valid Loss 3.773\n",
      "([['D EH1 R IH0 K S', 'D ER1 IH0 K S']], array([[-2.78148673, -3.65953329]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss 3.030, Valid Loss 2.622\n",
      "([['D EH1 R IH0 K S', 'D IH1 R IH0 K S']], array([[-1.70589991, -3.50420103]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss 2.351, Valid Loss 2.346\n",
      "([['D EH1 R IH0 K S', 'D EH1 R IH0 K IH0 NG']], array([[-2.06760684, -2.8355913 ]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss 2.042, Valid Loss 2.131\n",
      "([['D EH1 R IH0 K S', 'D EH1 R IH0 K IH0 K S']], array([[-3.94403873, -4.09484099]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss 1.838, Valid Loss 2.029\n",
      "([['D EH1 R IH0 K S', 'D EH1 R IH0 K IH0 S']], array([[-3.633751  , -4.33726468]]))\n"
     ]
    }
   ],
   "source": [
    "cmu_best_val_loss = None\n",
    "cmu_checkpoint = None\n",
    "for e in range(5):\n",
    "    loss = train.run_one_epoch(cmu_train_dataset,\n",
    "                               True,\n",
    "                               from_script='en',\n",
    "                               to_script='cmu',\n",
    "                               encoder=encoder,\n",
    "                               decoder=cmu_decoder,\n",
    "                               optimizer=optimizer,\n",
    "                               loss_function=loss_function)\n",
    "    valid_loss = train.run_one_epoch(cmu_valid_dataset,\n",
    "                                     False,\n",
    "                                     from_script='en',\n",
    "                                     to_script='cmu',\n",
    "                                     encoder=encoder,\n",
    "                                     decoder=cmu_decoder,\n",
    "                                     loss_function=loss_function)\n",
    "    if cmu_best_val_loss is None or valid_loss < cmu_best_val_loss:\n",
    "        cmu_best_val_loss = valid_loss\n",
    "        cmu_checkpoint = checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    print(\"Epoch {}: Train Loss {:.3f}, Valid Loss {:.3f}\".format(e, loss, valid_loss))\n",
    "    print(decode.transliterate(input_strs=['derick'],\n",
    "                               from_script='en',\n",
    "                               to_script='cmu',\n",
    "                               encoder=encoder,\n",
    "                               decoder=cmu_decoder,\n",
    "                               k_best=2,\n",
    "                               decoding_method=decode.beam_search_decode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.029012, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "checkpoint.restore(cmu_checkpoint).assert_consumed()\n",
    "print(train.run_one_epoch(cmu_valid_dataset,\n",
    "                          False,\n",
    "                          from_script='en',\n",
    "                          to_script='cmu',\n",
    "                          encoder=encoder,\n",
    "                          decoder=cmu_decoder,\n",
    "                          loss_function=loss_function))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def run_some_epochs(epochs):\n",
    "    checkpoint_path = None\n",
    "    best_val_loss = None\n",
    "    for e in range(epochs):\n",
    "        loss = train.run_one_epoch(eob_train_dataset,\n",
    "                                   True,\n",
    "                                   from_script='en',\n",
    "                                   to_script='ja',\n",
    "                                   encoder=encoder,\n",
    "                                   decoder=ja_decoder,\n",
    "                                   optimizer=optimizer,\n",
    "                                   loss_function=loss_function)\n",
    "        valid_loss = train.run_one_epoch(eob_valid_dataset,\n",
    "                                         False,\n",
    "                                         from_script='en',\n",
    "                                         to_script='ja',\n",
    "                                         encoder=encoder,\n",
    "                                         decoder=ja_decoder,\n",
    "                                         loss_function=loss_function)\n",
    "        print(\"Epoch {}: Train Loss {:.3f}, Valid Loss {:.3f}\".format(e, loss, valid_loss))\n",
    "        print(decode.transliterate(input_strs=['derick'],\n",
    "                                       from_script='en',\n",
    "                                       to_script='ja',\n",
    "                                       encoder=encoder,\n",
    "                                       decoder=ja_decoder,\n",
    "                                       k_best=2,\n",
    "                                       decoding_method=decode.beam_search_decode))\n",
    "        if best_val_loss is None or valid_loss < best_val_loss:\n",
    "            best_val_loss = valid_loss\n",
    "            checkpoint_path = checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "        else:\n",
    "            break\n",
    "    return checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss 14.270, Valid Loss 8.867\n",
      "([['ディリク', 'デリック']], array([[-4.45877628, -4.71044749]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss 6.988, Valid Loss 6.035\n",
      "([['デリック', 'ディック']], array([[-3.42910734, -3.80303085]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss 5.239, Valid Loss 5.497\n",
      "([['ディック', 'ディリック']], array([[-3.83938559, -3.85506754]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss 4.294, Valid Loss 5.091\n",
      "([['デリック', 'ダリック']], array([[-2.9712317 , -3.96789042]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss 3.578, Valid Loss 4.916\n",
      "([['デリック', 'ディック']], array([[-3.5558138 , -3.97427858]]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=110873345, shape=(), dtype=float32, numpy=4.8619604>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for layer in encoder.layers:\n",
    "    layer.trainable = False\n",
    "checkpoint_path = run_some_epochs(5)\n",
    "checkpoint.restore(checkpoint_path).assert_consumed()\n",
    "train.run_one_epoch(eob_valid_dataset,\n",
    "                    False,\n",
    "                    from_script='en',\n",
    "                    to_script='ja',\n",
    "                    encoder=encoder,\n",
    "                    decoder=ja_decoder,\n",
    "                    loss_function=loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss 2.976, Valid Loss 4.879\n",
      "([['ダリック', 'デリック']], array([[-2.31314964, -2.89069125]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss 2.511, Valid Loss 4.724\n",
      "([['ダリック', 'デリック']], array([[-2.69333865, -3.04696838]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss 2.103, Valid Loss 4.792\n",
      "([['ダリック', 'ダリキック']], array([[-2.77918531, -3.77301646]]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=122401708, shape=(), dtype=float32, numpy=4.6661897>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for layer in encoder.layers:\n",
    "    layer.trainable = True\n",
    "checkpoint_path = run_some_epochs(10)\n",
    "checkpoint.restore(checkpoint_path).assert_consumed()\n",
    "train.run_one_epoch(eob_valid_dataset,\n",
    "                    False,\n",
    "                    from_script='en',\n",
    "                    to_script='ja',\n",
    "                    encoder=encoder,\n",
    "                    decoder=ja_decoder,\n",
    "                    loss_function=loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "valid_df = pd.read_csv('../data/split/eob_pairs_valid.csv',\n",
    "                       keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49069003285870755"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = decode.transliterate(input_strs=valid_df['en'].values,\n",
    "                     from_script='en',\n",
    "                     to_script='ja',\n",
    "                     encoder=encoder,\n",
    "                     decoder=ja_decoder,\n",
    "                     k_best=20,\n",
    "                     num_beams=40,\n",
    "                     decoding_method=decode.beam_search_decode)\n",
    "evaluate.top_k_accuracy(valid_df['ja'].values, tr, k=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-1.13]",
   "name": "conda-env-tf-1.13-py"
  },
  "name": "Untitled.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
