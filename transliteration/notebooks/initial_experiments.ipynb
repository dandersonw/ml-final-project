{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.enable_eager_execution()\n",
    "import importlib\n",
    "import os\n",
    "\n",
    "from transliteration import data, train, model_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "importlib.reload(data)\n",
    "batch_size = 128\n",
    "train_dataset = data.make_dataset('../data/tfrecord/train.tfrecord',\n",
    "                                  batch_size=batch_size)\n",
    "valid_dataset = data.make_dataset('../data/tfrecord/valid.tfrecord',\n",
    "                                  batch_size=batch_size)\n",
    "test_dataset = data.make_dataset('../data/tfrecord/test.tfrecord',\n",
    "                                 batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = 1 - np.equal(real, 0)\n",
    "  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "importlib.reload(model_one)\n",
    "encoder_config = model_one.Config(lstm_size=60,\n",
    "                                  embedding_size=60,\n",
    "                                  attention_size=30,\n",
    "                                  vocab_size=data.ENGLISH_VOCAB_SIZE)\n",
    "decoder_config = model_one.Config(lstm_size=60,\n",
    "                                  embedding_size=30,\n",
    "                                  attention_size=30,\n",
    "                                  vocab_size=data.KATAKANA_VOCAB_SIZE)\n",
    "encoder = model_one.Encoder(encoder_config)\n",
    "decoder = model_one.Decoder(decoder_config)\n",
    "start_token=data.intern_katakana_char('<start>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "importlib.reload(train)\n",
    "best_val_loss = None\n",
    "for e in range(200):\n",
    "    loss = train.run_one_epoch(train_dataset,\n",
    "                               True,\n",
    "                               input_key='en',\n",
    "                               output_key='ja',\n",
    "                               encoder=encoder,\n",
    "                               decoder=decoder,\n",
    "                               optimizer=optimizer,\n",
    "                               loss_function=loss_function,\n",
    "                               start_token=start_token)\n",
    "    valid_loss = train.run_one_epoch(valid_dataset,\n",
    "                                     False,\n",
    "                                     input_key='en',\n",
    "                                     output_key='ja',\n",
    "                                     encoder=encoder,\n",
    "                                     decoder=decoder,\n",
    "                                     loss_function=loss_function,\n",
    "                                     start_token=start_token)\n",
    "    if best_val_loss is None or valid_loss < best_val_loss:\n",
    "        best_val_loss = valid_loss\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    print(\"Epoch {}: Train Loss {:.3f}, Valid Loss {:.3f}\".format(e, loss, valid_loss))\n",
    "    print(train.transliterate_single(input_str='derick',\n",
    "                           intern_input_fun=data.intern_en_char,\n",
    "                           deintern_output_fun=data.deintern_katakana_char,\n",
    "                           encoder=encoder,\n",
    "                           decoder=decoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7f8cf20337f0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(checkpoint_prefix + '-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ウィリアムム']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(train)\n",
    "importlib.reload(data)\n",
    "train.transliterate_single(input_str='william',\n",
    "                           intern_input_fun=data.intern_en_char,\n",
    "                           deintern_output_fun=data.deintern_katakana_char,\n",
    "                           encoder=encoder,\n",
    "                           decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.intern_katakana_char('<end>')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-1.13]",
   "name": "conda-env-tf-1.13-py"
  },
  "name": "Untitled.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
