{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.enable_eager_execution()\n",
    "import importlib\n",
    "import os\n",
    "\n",
    "from transliteration import data, train, model_one, script, decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "importlib.reload(data)\n",
    "batch_size = 256\n",
    "train_dataset = data.make_dataset('../data/tfrecord/train.tfrecord',\n",
    "                                  from_script='en',\n",
    "                                  to_script='ja',\n",
    "                                  batch_size=batch_size)\n",
    "valid_dataset = data.make_dataset('../data/tfrecord/valid.tfrecord',\n",
    "                                  from_script='en',\n",
    "                                  to_script='ja',\n",
    "                                  batch_size=batch_size)\n",
    "test_dataset = data.make_dataset('../data/tfrecord/test.tfrecord',\n",
    "                                 from_script='en',\n",
    "                                 to_script='ja',\n",
    "                                 batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "cmu_train_dataset = data.make_dataset('../data/tfrecord/cmu_train.tfrecord',\n",
    "                                  from_script='en',\n",
    "                                  to_script='cmu',\n",
    "                                  batch_size=batch_size)\n",
    "cmu_valid_dataset = data.make_dataset('../data/tfrecord/cmu_valid.tfrecord',\n",
    "                                  from_script='en',\n",
    "                                  to_script='cmu',\n",
    "                                  batch_size=batch_size)\n",
    "cmu_test_dataset = data.make_dataset('../data/tfrecord/cmu_test.tfrecord',\n",
    "                                 from_script='en',\n",
    "                                 to_script='cmu',\n",
    "                                 batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "importlib.reload(model_one)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred)\n",
    "    return tf.reduce_mean(loss_ * mask)\n",
    "\n",
    "encoder_config = model_one.Config(lstm_size=120,\n",
    "                                  embedding_size=60,\n",
    "                                  attention_size=None,\n",
    "                                  vocab_size=script.SCRIPTS['en'].vocab_size)\n",
    "decoder_config = model_one.Config(lstm_size=80,\n",
    "                                  embedding_size=60,\n",
    "                                  attention_size=60,\n",
    "                                  attention='monotonic_bahdanau',\n",
    "                                  vocab_size=script.SCRIPTS['ja'].vocab_size)\n",
    "cmu_decoder_config = model_one.Config(lstm_size=80,\n",
    "                                  embedding_size=60,\n",
    "                                  attention_size=60,\n",
    "                                  attention='monotonic_bahdanau',\n",
    "                                  vocab_size=script.SCRIPTS['cmu'].vocab_size)\n",
    "encoder = model_one.Encoder(encoder_config)\n",
    "decoder = model_one.Decoder(decoder_config)\n",
    "cmu_decoder = model_one.Decoder(cmu_decoder_config)\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder,\n",
    "                                 cmu_decoder=cmu_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss 16.177, Valid Loss 7.804\n",
      "([['D ER0 IH0 K K AH0 K', 'D ER0 IH0 K K AH0 K AH0 S']], array([[ -8.2832819 , -11.33341632]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss 5.812, Valid Loss 4.820\n",
      "([['D EH1 R IH0 K AH0 S', 'D EH1 R IH0 K AH0 K AH0 N']], array([[-6.2494958 , -8.99595803]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss 4.102, Valid Loss 3.865\n",
      "([['D EH1 R IH0 K AH0 S', 'D EH1 R IH0 K AH0 K AH0 N']], array([[-4.97263764, -7.36846613]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss 3.398, Valid Loss 3.263\n",
      "([['D EH1 R IH0 K AH0 K ER0', 'D EH1 R IH0 K AH0 K AH0 N']], array([[-6.09805015, -7.05461708]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss 2.992, Valid Loss 3.024\n",
      "([['D EH1 R IH0 K AH0 CH ER0', 'D EH1 R IH0 K AH0 K AH0 N']], array([[-5.87977612, -8.12592036]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss 2.784, Valid Loss 2.739\n",
      "([['D EH1 R IH0 K AH0 CH ER0', 'D EH1 R IH0 K AH0 L Z']], array([[-5.35534295, -5.89101115]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss 2.526, Valid Loss 2.623\n",
      "([['D EH1 R IH0 K AH0 CH ER0', 'D EH1 R IH0 K AH0 K AH0 L Z']], array([[-5.45755468, -9.41242491]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss 2.376, Valid Loss 2.515\n",
      "([['D EH1 R IH0 K AH0 CH ER0', 'D EH1 R IH0 K AH0 K AH0 L D ER0']], array([[ -5.4120316 , -11.06813771]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss 2.272, Valid Loss 2.366\n",
      "([['D EH1 R IH0 K AH0 CH ER0', 'D EH1 R IH0 K AH0 NG K ER0']], array([[-5.50152449, -6.60748948]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss 2.169, Valid Loss 2.286\n",
      "([['D EH1 R IH0 K ER0', 'D EH1 R IH0 K AH0 CH ER0']], array([[-3.5732613, -4.9583194]]))\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(train)\n",
    "best_val_loss = None\n",
    "for e in range(10):\n",
    "    loss = train.run_one_epoch(cmu_train_dataset,\n",
    "                               True,\n",
    "                               from_script='en',\n",
    "                               to_script='cmu',\n",
    "                               encoder=encoder,\n",
    "                               decoder=cmu_decoder,\n",
    "                               optimizer=optimizer,\n",
    "                               loss_function=loss_function)\n",
    "    valid_loss = train.run_one_epoch(cmu_valid_dataset,\n",
    "                                     False,\n",
    "                                     from_script='en',\n",
    "                                     to_script='cmu',\n",
    "                                     encoder=encoder,\n",
    "                                     decoder=cmu_decoder,\n",
    "                                     loss_function=loss_function)\n",
    "    if best_val_loss is None or valid_loss < best_val_loss:\n",
    "        best_val_loss = valid_loss\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    print(\"Epoch {}: Train Loss {:.3f}, Valid Loss {:.3f}\".format(e, loss, valid_loss))\n",
    "    print(decode.transliterate(input_strs=['derick'],\n",
    "                               from_script='en',\n",
    "                               to_script='cmu',\n",
    "                               encoder=encoder,\n",
    "                               decoder=cmu_decoder,\n",
    "                               k_best=2,\n",
    "                               decoding_method=decode.beam_search_decode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss 14.830, Valid Loss 10.378\n",
      "([['デリック', 'デリクク']], array([[-4.42585427, -4.96765587]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss 8.380, Valid Loss 7.523\n",
      "([['デリック', 'デリックル']], array([[-3.11422116, -5.02311182]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss 6.674, Valid Loss 6.537\n",
      "([['デリック', 'デリックス']], array([[-3.37739228, -4.80727522]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss 5.952, Valid Loss 6.151\n",
      "([['デリック', 'デリックス']], array([[-2.88655928, -4.43254995]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss 5.427, Valid Loss 5.988\n",
      "([['デリック', 'デリックス']], array([[-2.59951185, -4.37641649]]))\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(train)\n",
    "best_val_loss = None\n",
    "for e in range(5):\n",
    "    loss = train.run_one_epoch(train_dataset,\n",
    "                               True,\n",
    "                               from_script='en',\n",
    "                               to_script='ja',\n",
    "                               encoder=encoder,\n",
    "                               decoder=decoder,\n",
    "                               train_encoder=False,\n",
    "                               optimizer=optimizer,\n",
    "                               loss_function=loss_function)\n",
    "    valid_loss = train.run_one_epoch(valid_dataset,\n",
    "                                     False,\n",
    "                                     from_script='en',\n",
    "                                     to_script='ja',\n",
    "                                     encoder=encoder,\n",
    "                                     decoder=decoder,\n",
    "                                     loss_function=loss_function)\n",
    "    if best_val_loss is None or valid_loss < best_val_loss:\n",
    "        best_val_loss = valid_loss\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    print(\"Epoch {}: Train Loss {:.3f}, Valid Loss {:.3f}\".format(e, loss, valid_loss))\n",
    "    print(decode.transliterate(input_strs=['derick'],\n",
    "                               from_script='en',\n",
    "                               to_script='ja',\n",
    "                               encoder=encoder,\n",
    "                               decoder=decoder,\n",
    "                               k_best=2,\n",
    "                               decoding_method=decode.beam_search_decode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss 3.061, Valid Loss 4.737\n",
      "([['デリック', 'デリク']], array([[-2.0089123, -3.2298286]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss 2.940, Valid Loss 4.718\n",
      "([['デリック', 'デリク']], array([[-1.90779161, -3.51031125]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss 2.837, Valid Loss 4.761\n",
      "([['デリック', 'デリク']], array([[-1.91291335, -2.73543338]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss 2.738, Valid Loss 4.881\n",
      "([['デリック', 'デリク']], array([[-1.55329115, -2.82706814]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss 2.653, Valid Loss 4.749\n",
      "([['デリック', 'デリックス']], array([[-1.62591756, -3.16085919]]))\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(train)\n",
    "best_val_loss = None\n",
    "for e in range(5):\n",
    "    loss = train.run_one_epoch(train_dataset,\n",
    "                               True,\n",
    "                               from_script='en',\n",
    "                               to_script='ja',\n",
    "                               encoder=encoder,\n",
    "                               decoder=decoder,\n",
    "                               train_encoder=True,\n",
    "                               optimizer=optimizer,\n",
    "                               loss_function=loss_function)\n",
    "    valid_loss = train.run_one_epoch(valid_dataset,\n",
    "                                     False,\n",
    "                                     from_script='en',\n",
    "                                     to_script='ja',\n",
    "                                     encoder=encoder,\n",
    "                                     decoder=decoder,\n",
    "                                     loss_function=loss_function)\n",
    "    if best_val_loss is None or valid_loss < best_val_loss:\n",
    "        best_val_loss = valid_loss\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    print(\"Epoch {}: Train Loss {:.3f}, Valid Loss {:.3f}\".format(e, loss, valid_loss))\n",
    "    print(decode.transliterate(input_strs=['derick'],\n",
    "                               from_script='en',\n",
    "                               to_script='ja',\n",
    "                               encoder=encoder,\n",
    "                               decoder=decoder,\n",
    "                               k_best=2,\n",
    "                               decoding_method=decode.beam_search_decode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=129273772, shape=(), dtype=float32, numpy=4.719208>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(checkpoint_prefix + '-26').assert_consumed()\n",
    "train.run_one_epoch(valid_dataset,\n",
    "                    False,\n",
    "                    from_script='en',\n",
    "                    to_script='ja',\n",
    "                    encoder=encoder,\n",
    "                    decoder=decoder,\n",
    "                    loss_function=loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['ルーム', 'ルームズ']], array([[-1.43902729, -3.161433  ]]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(decode)\n",
    "decode.transliterate(input_strs=['room'],\n",
    "                     from_script='en',\n",
    "                     to_script='ja',\n",
    "                     encoder=encoder,\n",
    "                     decoder=decoder,\n",
    "                     k_best=2,\n",
    "                     decoding_method=decode.beam_search_decode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-1.13]",
   "name": "conda-env-tf-1.13-py"
  },
  "name": "initial_experiments.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
