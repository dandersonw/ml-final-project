{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.enable_eager_execution()\n",
    "import importlib\n",
    "import os\n",
    "\n",
    "from transliteration import data, train, model_one, script, decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "importlib.reload(data)\n",
    "batch_size = 128\n",
    "train_dataset = data.make_dataset('../data/tfrecord/train.tfrecord',\n",
    "                                  from_script='en',\n",
    "                                  to_script='ja',\n",
    "                                  batch_size=batch_size)\n",
    "valid_dataset = data.make_dataset('../data/tfrecord/valid.tfrecord',\n",
    "                                  from_script='en',\n",
    "                                  to_script='ja',\n",
    "                                  batch_size=batch_size)\n",
    "test_dataset = data.make_dataset('../data/tfrecord/test.tfrecord',\n",
    "                                 from_script='en',\n",
    "                                 to_script='ja',\n",
    "                                 batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred)\n",
    "    return tf.reduce_mean(loss_ * mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "importlib.reload(model_one)\n",
    "encoder_config = model_one.Config(lstm_size=120,\n",
    "                                  embedding_size=60,\n",
    "                                  attention_size=None,\n",
    "                                  vocab_size=script.SCRIPTS['en'].vocab_size)\n",
    "decoder_config = model_one.Config(lstm_size=80,\n",
    "                                  embedding_size=30,\n",
    "                                  attention_size=30,\n",
    "                                  vocab_size=script.SCRIPTS['ja'].vocab_size)\n",
    "encoder = model_one.Encoder(encoder_config)\n",
    "decoder = model_one.Decoder(decoder_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "importlib.reload(train)\n",
    "best_val_loss = None\n",
    "for e in range(200):\n",
    "    loss = train.run_one_epoch(train_dataset,\n",
    "                               True,\n",
    "                               from_script='en',\n",
    "                               to_script='ja',\n",
    "                               encoder=encoder,\n",
    "                               decoder=decoder,\n",
    "                               optimizer=optimizer,\n",
    "                               loss_function=loss_function)\n",
    "    valid_loss = train.run_one_epoch(valid_dataset,\n",
    "                                     False,\n",
    "                                     from_script='en',\n",
    "                                     to_script='ja',\n",
    "                                     encoder=encoder,\n",
    "                                     decoder=decoder,\n",
    "                                     loss_function=loss_function)\n",
    "    if best_val_loss is None or valid_loss < best_val_loss:\n",
    "        best_val_loss = valid_loss\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    print(\"Epoch {}: Train Loss {:.3f}, Valid Loss {:.3f}\".format(e, loss, valid_loss))\n",
    "    print(decode.transliterate(input_strs=['derick'],\n",
    "                               from_script='en',\n",
    "                               to_script='ja',\n",
    "                               encoder=encoder,\n",
    "                               decoder=decoder,\n",
    "                               k_best=2,\n",
    "                               decoding*_method=decode.beam_search_decode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7f430fa68d30>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(checkpoint_prefix + '-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['ヘルメト', 'ヘルメット']], array([[-1.66559254, -2.45833173]]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(decode)\n",
    "decode.transliterate(input_strs=['helmet'],\n",
    "                     from_script='en',\n",
    "                     to_script='ja',\n",
    "                     encoder=encoder,\n",
    "                     decoder=decoder,\n",
    "                     k_best=2,\n",
    "                     decoding_method=decode.beam_search_decode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-1.13]",
   "name": "conda-env-tf-1.13-py"
  },
  "name": "initial_experiments.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
