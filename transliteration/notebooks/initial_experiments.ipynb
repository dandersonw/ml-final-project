{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.enable_eager_execution()\n",
    "import importlib\n",
    "import os\n",
    "\n",
    "from transliteration import data, train, model_one, script, decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "importlib.reload(data)\n",
    "batch_size = 256\n",
    "train_dataset = data.make_dataset('../data/tfrecord/train.tfrecord',\n",
    "                                  from_script='en',\n",
    "                                  to_script='ja',\n",
    "                                  batch_size=batch_size)\n",
    "valid_dataset = data.make_dataset('../data/tfrecord/valid.tfrecord',\n",
    "                                  from_script='en',\n",
    "                                  to_script='ja',\n",
    "                                  batch_size=batch_size)\n",
    "test_dataset = data.make_dataset('../data/tfrecord/test.tfrecord',\n",
    "                                 from_script='en',\n",
    "                                 to_script='ja',\n",
    "                                 batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "cmu_train_dataset = data.make_dataset('../data/tfrecord/cmu_train.tfrecord',\n",
    "                                  from_script='en',\n",
    "                                  to_script='cmu',\n",
    "                                  batch_size=batch_size)\n",
    "cmu_valid_dataset = data.make_dataset('../data/tfrecord/cmu_valid.tfrecord',\n",
    "                                  from_script='en',\n",
    "                                  to_script='cmu',\n",
    "                                  batch_size=batch_size)\n",
    "cmu_test_dataset = data.make_dataset('../data/tfrecord/cmu_test.tfrecord',\n",
    "                                 from_script='en',\n",
    "                                 to_script='cmu',\n",
    "                                 batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "importlib.reload(model_one)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred)\n",
    "    return tf.reduce_mean(loss_ * mask)\n",
    "\n",
    "encoder_config = model_one.Config(lstm_size=120,\n",
    "                                  embedding_size=60,\n",
    "                                  attention_size=None,\n",
    "                                  vocab_size=script.SCRIPTS['en'].vocab_size)\n",
    "decoder_config = model_one.Config(lstm_size=80,\n",
    "                                  embedding_size=60,\n",
    "                                  attention_size=60,\n",
    "                                  attention='monotonic_bahdanau',\n",
    "                                  vocab_size=script.SCRIPTS['ja'].vocab_size)\n",
    "cmu_decoder_config = model_one.Config(lstm_size=80,\n",
    "                                  embedding_size=60,\n",
    "                                  attention_size=60,\n",
    "                                  vocab_size=script.SCRIPTS['cmu'].vocab_size)\n",
    "encoder = model_one.Encoder(encoder_config)\n",
    "decoder = model_one.Decoder(decoder_config)\n",
    "cmu_decoder = model_one.Decoder(cmu_decoder_config)\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder,\n",
    "                                 cmu_decoder=cmu_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "importlib.reload(train)\n",
    "best_val_loss = None\n",
    "for e in range(200):\n",
    "    loss = train.run_one_epoch(cmu_train_dataset,\n",
    "                               True,\n",
    "                               from_script='en',\n",
    "                               to_script='cmu',\n",
    "                               encoder=encoder,\n",
    "                               decoder=cmu_decoder,\n",
    "                               optimizer=optimizer,\n",
    "                               loss_function=loss_function)\n",
    "    valid_loss = train.run_one_epoch(cmu_valid_dataset,\n",
    "                                     False,\n",
    "                                     from_script='en',\n",
    "                                     to_script='cmu',\n",
    "                                     encoder=encoder,\n",
    "                                     decoder=cmu_decoder,\n",
    "                                     loss_function=loss_function)\n",
    "    if best_val_loss is None or valid_loss < best_val_loss:\n",
    "        best_val_loss = valid_loss\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    print(\"Epoch {}: Train Loss {:.3f}, Valid Loss {:.3f}\".format(e, loss, valid_loss))\n",
    "    print(decode.transliterate(input_strs=['derick'],\n",
    "                               from_script='en',\n",
    "                               to_script='cmu',\n",
    "                               encoder=encoder,\n",
    "                               decoder=cmu_decoder,\n",
    "                               k_best=2,\n",
    "                               decoding_method=decode.beam_search_decode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss 21.233, Valid Loss 19.356\n",
      "([['ーー', 'ンー']], array([[-10.15840602, -10.16695213]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss 18.729, Valid Loss 18.756\n",
      "([['アルーー', 'アルーーー']], array([[-17.87236977, -20.87293935]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss 18.207, Valid Loss 18.049\n",
      "([['アランラン', 'アリンラン']], array([[-20.51289344, -20.73459411]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss 17.319, Valid Loss 17.099\n",
      "([['アラント', 'アリント']], array([[-15.59250736, -15.73684573]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss 15.924, Valid Loss 15.251\n",
      "([['ディリータート', 'ディアラータート']], array([[-22.44977331, -25.17119241]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss 14.076, Valid Loss 13.448\n",
      "([['ディリックト', 'ディレックト']], array([[-13.81076324, -13.97866535]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss 12.183, Valid Loss 12.144\n",
      "([['ディックト', 'ディリックト']], array([[ -9.55809736, -11.36953717]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss 10.649, Valid Loss 10.246\n",
      "([['ドリック', 'ディリック']], array([[-7.21610558, -8.3488006 ]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss 9.551, Valid Loss 9.673\n",
      "([['ドリック', 'ディック']], array([[-5.58030021, -6.59443706]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss 8.830, Valid Loss 8.823\n",
      "([['ディック', 'ドリック']], array([[-5.26165286, -5.88261306]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss 8.088, Valid Loss 8.036\n",
      "([['ディック', 'デリック']], array([[-5.06439063, -5.16589776]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss 7.386, Valid Loss 7.452\n",
      "([['ディック', 'ディリック']], array([[-4.80800387, -5.33656245]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss 6.938, Valid Loss 7.324\n",
      "([['デリック', 'ディック']], array([[-4.12823898, -4.91474569]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss 6.660, Valid Loss 6.921\n",
      "([['デリック', 'ディック']], array([[-3.88129631, -3.95454511]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss 6.299, Valid Loss 6.613\n",
      "([['デリック', 'ディック']], array([[-3.41025676, -4.54222052]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss 6.018, Valid Loss 6.360\n",
      "([['デリック', 'デリク']], array([[-3.21972992, -4.00256772]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss 5.770, Valid Loss 6.254\n",
      "([['デリック', 'デリス']], array([[-2.8981768, -4.1935067]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss 5.559, Valid Loss 6.168\n",
      "([['デリック', 'デリク']], array([[-2.7408665 , -3.82106943]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss 5.383, Valid Loss 5.923\n",
      "([['デリック', 'デリク']], array([[-2.51655251, -3.69024467]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss 5.235, Valid Loss 5.826\n",
      "([['デリック', 'デリク']], array([[-2.33607227, -3.59429201]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss 5.113, Valid Loss 5.792\n",
      "([['デリック', 'デリク']], array([[-2.31403704, -3.71934621]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss 4.943, Valid Loss 5.666\n",
      "([['デリック', 'デリク']], array([[-2.30544494, -3.50285442]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train Loss 4.805, Valid Loss 5.586\n",
      "([['デリック', 'デリク']], array([[-2.11921783, -3.3709477 ]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train Loss 4.667, Valid Loss 5.509\n",
      "([['デリック', 'デリク']], array([[-2.57041283, -3.54113092]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train Loss 4.518, Valid Loss 5.397\n",
      "([['デリック', 'デリク']], array([[-2.03805599, -3.50342301]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Train Loss 4.440, Valid Loss 5.440\n",
      "([['デリック', 'デリックス']], array([[-1.98899063, -3.46273818]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Train Loss 4.310, Valid Loss 5.323\n",
      "([['デリック', 'デリックス']], array([[-1.94289263, -3.15559326]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Train Loss 4.176, Valid Loss 5.329\n",
      "([['デリック', 'デリックス']], array([[-2.01173321, -3.28825078]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Train Loss 4.093, Valid Loss 5.239\n",
      "([['デリック', 'デリックス']], array([[-1.83368374, -3.17345879]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train Loss 3.984, Valid Loss 5.249\n",
      "([['デリック', 'デリックス']], array([[-1.91276402, -3.02227522]]))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ca48478c3df0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                \u001b[0mtrain_encoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                                loss_function=loss_function)\n\u001b[0m\u001b[1;32m     13\u001b[0m     valid_loss = train.run_one_epoch(valid_dataset,\n\u001b[1;32m     14\u001b[0m                                      \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/ml-final-project/transliteration/transliteration/train.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(dataset, train, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mevaluate_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/ml-final-project/transliteration/transliteration/train.py\u001b[0m in \u001b[0;36mtrain_one_batch\u001b[0;34m(batch, from_script, to_script, encoder, decoder, train_encoder, train_decoder, optimizer, loss_function)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain_decoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MinimumGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_MinimumGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m   \u001b[0;34m\"\"\"Returns grad*(x < y, x >= y) with type of grad.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_MaximumMinimumGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mless_equal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MaximumMinimumGrad\u001b[0;34m(op, grad, selector_op)\u001b[0m\n\u001b[1;32m   1063\u001b[0m   \u001b[0mxmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m   \u001b[0mrx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_gradient_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m   \u001b[0mxgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m   \u001b[0mygrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m   \u001b[0mgx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(condition, x, y, name)\u001b[0m\n\u001b[1;32m   3202\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3203\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3204\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3205\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3206\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must both be non-None or both be None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(condition, x, y, name)\u001b[0m\n\u001b[1;32m   8047\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m   8048\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Select\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8049\u001b[0;31m         _ctx._post_execution_callbacks, condition, x, y)\n\u001b[0m\u001b[1;32m   8050\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8051\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "importlib.reload(train)\n",
    "best_val_loss = None\n",
    "for e in range(200):\n",
    "    loss = train.run_one_epoch(train_dataset,\n",
    "                               True,\n",
    "                               from_script='en',\n",
    "                               to_script='ja',\n",
    "                               encoder=encoder,\n",
    "                               decoder=decoder,\n",
    "                               train_encoder=True,\n",
    "                               optimizer=optimizer,\n",
    "                               loss_function=loss_function)\n",
    "    valid_loss = train.run_one_epoch(valid_dataset,\n",
    "                                     False,\n",
    "                                     from_script='en',\n",
    "                                     to_script='ja',\n",
    "                                     encoder=encoder,\n",
    "                                     decoder=decoder,\n",
    "                                     loss_function=loss_function)\n",
    "    if best_val_loss is None or valid_loss < best_val_loss:\n",
    "        best_val_loss = valid_loss\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    print(\"Epoch {}: Train Loss {:.3f}, Valid Loss {:.3f}\".format(e, loss, valid_loss))\n",
    "    print(decode.transliterate(input_strs=['derick'],\n",
    "                               from_script='en',\n",
    "                               to_script='ja',\n",
    "                               encoder=encoder,\n",
    "                               decoder=decoder,\n",
    "                               k_best=2,\n",
    "                               decoding_method=decode.beam_search_decode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7f430fa68d30>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(checkpoint_prefix + '-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['アルモール', 'アーモーロー']], array([[-5.08133715, -5.74089668]]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(decode)\n",
    "decode.transliterate(input_strs=['armor'],\n",
    "                     from_script='en',\n",
    "                     to_script='ja',\n",
    "                     encoder=encoder,\n",
    "                     decoder=decoder,\n",
    "                     k_best=2,\n",
    "                     decoding_method=decode.beam_search_decode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-1.13]",
   "name": "conda-env-tf-1.13-py"
  },
  "name": "initial_experiments.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
